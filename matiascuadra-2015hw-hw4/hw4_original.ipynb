{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.10", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }, 
  "nbformat": 4, 
  "nbformat_minor": 0, 
  "cells": [
    {
      "source": [
        "#HW4: Is Remy the best cook in town?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "<img src=\"https://dl.dropboxusercontent.com/u/75194/remyego.png\" width=600 height=400/>"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Or, maybe, the question ought to be, could you combine the opinion of multiple Ego's into an electronic Egomaniac?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "##Writing recommender systems"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "In this homework, you will create a recommendation system for **restaurants** using [collaborative filtering](http://en.wikipedia.org/wiki/Collaborative_filtering) (CF). We will use Nearest Neighbor, Regression, and Matrix Factorization, and combine them using a simple ensemble method.\n", 
        "\n", 
        "The recommenders we write will not be particularly good or sophisticated, but we shall carry out the entire process someone who competed on the Netflix prize did, and on the way, we'll learn a lot about the gotchas and subtleties involved.\n", 
        "\n", 
        "The general structure of a recommendation system is that there are users and there are items. Here, the users are folks who have critiqued restaurants, while the items are the restaurants themselves. Users express explicit or implicit preferences towards certain items. CF thus relies on users' past behavior.\n", 
        "\n", 
        "The first part of any recommender system is to create baseline estimate of the ratings. Indeed, most of any rating can be usually explained by baseline estimates, and all other methods are an attempt (only sometimes successful, as we shall see) to improve these baselines. We shall create two baselines here: one based on estimating biases with sample averages, and the second one using regularized regression.\n", 
        "\n", 
        "There are two primary approaches to CF: neighborhood and latent factor model. The former is concerned with computing the relationships between items or between users. In the latter approach you have a model of hidden factors through which users and items are transformed to the same space. For example, if you are rating movies we may transform items into genre factors, and users into their preference for a particular genre.\n", 
        "\n", 
        "Factor models generally lead to more accurate recommenders. One of the reasons for this is the sparsity of the item-user matrix. Most users tend to rate barely one or two items. Latent factor models are more expressive, and fit fewer parameters. However, neighborhood models are more prevalent, as they have an intuitive aspect that appeals to users(if you liked this you will like that) and online(a new preference can be incorporated very quickly).\n", 
        "\n", 
        "Most recommenders today combine neighborhood CF with latent model based CF with matrix factorization approaches to latent factor models. \n", 
        "\n", 
        "In this homework you will learn:\n", 
        "\n", 
        "1. how to set up a sample based average baseline model\n", 
        "1. how to set up a collaborative filtering model, calculating a part of it using map-reduce methods\n", 
        "2. predict from the model using neighborhood based CF approaches\n", 
        "3. How to set up a baseline model using ridge regression \n", 
        "4. How to set up a matrix factorization using alternating least squares, and how to predict from it\n", 
        "4. How to combine the results from both models into a \"ensemble\" model using stacked regression\n", 
        "\n", 
        "In doing this homework you will be following the path that people who participated in the Netflix prize did. See https://datajobs.com/data-science-repo/Collaborative-Filtering-[Koren-and-Bell].pdf for advances in Collaborative Filtering brought about by the prize. Also see a very readable description of what went into the winning models [here](http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/)."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "##This homework is due Thursday, November 5th 2015, at 11:59PM EST."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We'll grade the last commit you make before the homework deadline. We will be looking for the file `hw4.ipynb`.\n", 
        "\n", 
        "**Start NOW. This is a long homework. Longer than HW2 and HW3.**"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 1, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline\n", 
        "import numpy as np\n", 
        "import scipy as sp\n", 
        "import matplotlib as mpl\n", 
        "import matplotlib.cm as cm\n", 
        "import matplotlib.pyplot as plt\n", 
        "import pandas as pd\n", 
        "pd.set_option('display.width', 500)\n", 
        "pd.set_option('display.max_columns', 100)\n", 
        "pd.set_option('display.notebook_repr_html', True)\n", 
        "import seaborn as sns\n", 
        "sns.set_style(\"whitegrid\")\n", 
        "sns.set_context(\"poster\")"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "#Table of Contents\n", 
        "* [HW4: Is Remy the best cook in town?](#HW4:-Is-Remy-the-best-cook-in-town?)\n", 
        "\t* [Writing recommender systems](#Writing-recommender-systems)\n", 
        "\t* [This homework is due Thursday, November 5th 2015, at 11:59PM EST.](#This-homework-is-due-Thursday,-November-5th-2015,-at-11:59PM-EST.)\n", 
        "\t\t* [User and Item based approaches](#User-and-Item-based-approaches)\n", 
        "\t* [Q1 EDA and the Baseline Model](#Q1-EDA-and-the-Baseline-Model)\n", 
        "\t\t* [Choosing a workable data frame](#Choosing-a-workable-data-frame)\n", 
        "\t\t\t* [1.1 Visualize the sparsity of the data set](#1.1-Visualize-the-sparsity-of-the-data-set)\n", 
        "\t\t* [Splitting into testing and training sets](#Splitting-into-testing-and-training-sets)\n", 
        "\t\t* [The baseline model](#The-baseline-model)\n", 
        "\t\t\t* [1.2 Calculate dictionaries of user and restaurant biases and plot histograms for them](#1.2-Calculate-dictionaries-of-user-and-restaurant-biases-and-plot-histograms-for-them)\n", 
        "\t\t* [Making baseline predictions](#Making-baseline-predictions)\n", 
        "\t\t\t* [1.3 Make the prediction from the `baseline` model](#1.3-Make-the-prediction-from-the-baseline-model)\n", 
        "\t* [Q2 Collaborative Filtering using kNN](#Q2-Collaborative-Filtering-using-kNN)\n", 
        "\t\t* [Setting up the Collaborative Filtering Model](#Setting-up-the-Collaborative-Filtering-Model)\n", 
        "\t\t\t* [Global Recommender](#Global-Recommender)\n", 
        "\t\t\t* [Local recommender](#Local-recommender)\n", 
        "\t\t* [Common Support](#Common-Support)\n", 
        "\t\t* [Calculating similarity](#Calculating-similarity)\n", 
        "\t\t\t* [2.1 Write a function `pearson_sim` that calculates the pearson similarity](#2.1-Write-a-function-pearson_sim-that-calculates-the-pearson-similarity)\n", 
        "\t\t\t* [Get Restaurant reviews](#Get-Restaurant-reviews)\n", 
        "\t\t* [Making a database of similarities](#Making-a-database-of-similarities)\n", 
        "\t\t* [Map reduce to populate database](#Map-reduce-to-populate-database)\n", 
        "\t\t\t* [2.2 Write `mapper2`](#2.2-Write-mapper2)\n", 
        "\t\t\t* [2.3 Write `reducer2`](#2.3-Write-reducer2)\n", 
        "\t\t\t* [Checking our work](#Checking-our-work)\n", 
        "\t\t\t* [Shrunk similarities](#Shrunk-similarities)\n", 
        "\t\t* [Calculating Nearest Neighbors](#Calculating-Nearest-Neighbors)\n", 
        "\t\t\t* [Similarity Distances](#Similarity-Distances)\n", 
        "\t\t\t* [We write a function `knearest`](#We-write-a-function-knearest)\n", 
        "\t\t\t* [2.4 Write a function to calculate the predicted rating for a user and restaurant](#2.4-Write-a-function-to-calculate-the-predicted-rating-for-a-user-and-restaurant)\n", 
        "\t\t* [Predicting on a user's neighborhood](#Predicting-on-a-user's-neighborhood)\n", 
        "\t\t\t* [An example](#An-example)\n", 
        "\t\t\t* [2.5 Plot graphs for a 2x2 grid of `k` and `reg` and interpret your results](#2.5-Plot-graphs-for-a-2x2-grid-of-k-and-reg-and-interpret-your-results)\n", 
        "\t\t\t* [2.6 Validate over `k` and `reg`](#2.6-Validate-over-k-and-reg)\n", 
        "\t\t\t* [2.7 Use `mintup` to calculate and plot the results on the test set and compare the results with the baseline model](#2.7-Use-mintup-to-calculate-and-plot-the-results-on-the-test-set-and-compare-the-results-with-the-baseline-model)\n", 
        "\t\t* [How to interpret these results?](#How-to-interpret-these-results?)\n", 
        "\t* [Q3: The Baseline model with regularization](#Q3:-The-Baseline-model-with-regularization)\n", 
        "\t\t* [Writing it as a Ridge Regression](#Writing-it-as-a-Ridge-Regression)\n", 
        "\t\t\t* [3.1 Write a function to produce the design matrix](#3.1-Write-a-function-to-produce-the-design-matrix)\n", 
        "\t\t\t* [3.2 Carry out the ridge regression!](#3.2-Carry-out-the-ridge-regression!)\n", 
        "\t\t* [Prediction!](#Prediction!)\n", 
        "\t\t\t* [3.3 Use this model to predict on the test set.](#3.3-Use-this-model-to-predict-on-the-test-set.)\n", 
        "\t\t\t* [3.4 Compare the results from the simple baseline model to the regularized baseline model.](#3.4-Compare-the-results-from-the-simple-baseline-model-to-the-regularized-baseline-model.)\n", 
        "\t* [Q4: Collaborative Filtering and The Latent Factor Model](#Q4:-Collaborative-Filtering-and-The-Latent-Factor-Model)\n", 
        "\t\t* [Model Overview](#Model-Overview)\n", 
        "\t\t* [Latent Factors by Matrix Factorization](#Latent-Factors-by-Matrix-Factorization)\n", 
        "\t\t\t* [Constructing the design matrix](#Constructing-the-design-matrix)\n", 
        "\t\t\t* [4.1 Write a function to compute `designq`.](#4.1-Write-a-function-to-compute-designq.)\n", 
        "\t\t* [WARNING: this part might take some hours to a day to run!](#WARNING:-this-part-might-take-some-hours-to-a-day-to-run!)\n", 
        "\t\t\t* [4.2 Make a prediction on the test set and plot your predictions compared to the basdeline model. Comment.](#4.2-Make-a-prediction-on-the-test-set-and-plot-your-predictions-compared-to-the-basdeline-model.-Comment.)\n", 
        "\t* [Q5 Combining results into an ensemble](#Q5-Combining-results-into-an-ensemble)\n", 
        "\t\t* [Adding a regularized baseline to kNN](#Adding-a-regularized-baseline-to-kNN)\n", 
        "\t\t\t* [5.1 Find the best fit `knn_r model` on the validation set and test sets](#5.1-Find-the-best-fit-knn_r-model-on-the-validation-set-and-test-sets)\n", 
        "\t\t\t* [5.2 Carry out an unregularized linear regression of the actual values against the three predictions](#5.2-Carry-out-an-unregularized-linear-regression-of-the-actual-values-against-the-three-predictions)\n", 
        "\t\t\t* [5.3 Get the same models on the test set and use linear regression to calculate the predictions. Comment](#5.3-Get-the-same-models-on-the-test-set-and-use-linear-regression-to-calculate-the-predictions.-Comment)\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### User and Item based approaches"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Original approaches to neighborhood based CF used user-user models. By this we mean that rating estimates are made from recorded ratings of like minded users. However, since most users tend to rate very few items, this is usually a losing proposition for explicit-rating based recommenders. Thus, most neighborhood based systems such as Amazon these days rely on item-item approaches. In these methods, a rating is estimated by other ratings made by the user on \"similar\" or \"nearby\" items: we have a K-Nearest-Neighbors algorithm, in effect."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "##Q1 EDA and the Baseline Model"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The data file here that you read in is a database of reviews, one review per row from the Yelp web site.  The columns are:\n", 
        "\n", 
        "* `user_id`: an opaque id for the reviewer\n", 
        "* `business_id`: an opaque id for the restaurant\n", 
        "* `date`: the date the review was made\n", 
        "* `review_id`: on opaque id for the review\n", 
        "* `stars`: the number of stars the user gave it\n", 
        "* `text`: the text of the review\n", 
        "* `business_avg`: the average rating this business got\n", 
        "* `business_review_count`: the number of reviews this business had\n", 
        "* `user_avg`: the average rating this user gave\n", 
        "* `user_review_count`: the number of reviews this user did.\n", 
        "\n", 
        "Users rate restaurants on a scale of 1-5. Even though this rating is integer valued, for the purposes of this assignment we shall treat it as a real number.\n", 
        "\n", 
        "We start with a 31MB in size data set and create a smaller one to start. The 31MB dataset is already too large for most of the computations in this notebook to complete within a few minutes, so we will need to pare it down."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 2, 
      "cell_type": "code", 
      "source": [
        "bigdf=pd.read_csv(\"bigdf.csv\")\n", 
        "bigdf.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The `recompute_frame` function below makes sure that a dataframe has the correct averages and counts for the ratings of businesses and users"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 3, 
      "cell_type": "code", 
      "source": [
        "def recompute_frame(ldf):\n", 
        "    \"\"\"\n", 
        "    takes a dataframe ldf, makes a copy of it, and returns the copy\n", 
        "    with all averages and review counts recomputed\n", 
        "    this is used when a frame is subsetted.\n", 
        "    \"\"\"\n", 
        "    ldfu=ldf.groupby('user_id')\n", 
        "    ldfb=ldf.groupby('business_id')\n", 
        "    user_avg=ldfu.stars.mean()\n", 
        "    user_review_count=ldfu.review_id.count()\n", 
        "    business_avg=ldfb.stars.mean()\n", 
        "    business_review_count=ldfb.review_id.count()\n", 
        "    nldf=ldf.copy()\n", 
        "    nldf.set_index(['business_id'], inplace=True)\n", 
        "    nldf['business_avg']=business_avg\n", 
        "    nldf['business_review_count']=business_review_count\n", 
        "    nldf.reset_index(inplace=True)\n", 
        "    nldf.set_index(['user_id'], inplace=True)\n", 
        "    nldf['user_avg']=user_avg\n", 
        "    nldf['user_review_count']=user_review_count\n", 
        "    nldf.reset_index(inplace=True)\n", 
        "    return nldf"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 4, 
      "cell_type": "code", 
      "source": [
        "bigdf=recompute_frame(bigdf)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "### Choosing a workable data frame"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The `compute_supports` function below computes the number of common users between two restaurants. This number is critical to understanding how similar restaurants are. With no users or just 1 user rating both restaurants in a pair, one cant really compare them. And the more users two restaurants have in common, the more we are likely to trust this number, which we shall call the **common support**. This similarity is important in neighborhood based models"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 5, 
      "cell_type": "code", 
      "source": [
        "def compute_supports(df):\n", 
        "    uuids=df.user_id.unique()\n", 
        "    rests = df.groupby('business_id').user_id.unique()\n", 
        "    bdict={}\n", 
        "    for e,v in zip(rests.index.values, rests.values):\n", 
        "        bdict[e] = np.array([item in v for item in uuids])\n", 
        "    restaurants=bdict.keys()\n", 
        "    supports=[]\n", 
        "    for i,rest1 in enumerate(restaurants):\n", 
        "        for j,rest2 in enumerate(restaurants):\n", 
        "            if  i < j:\n", 
        "                supmask = (bdict[rest1] & bdict[rest2])\n", 
        "                common_reviewers = np.sum(supmask)\n", 
        "                supports.append(common_reviewers)\n", 
        "    print \"mean support\",np.mean(supports), \"median support\", np.median(supports)\n", 
        "    return supports, bdict"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "The `make_smaller` function here is to take `bigdf` and make it smaller by having conditions on how many reviews a business must have, and how many reviews a user must have made.\n", 
        "\n", 
        "The former condition, having two restaurants have much common support, is more likely if we choose restaurants in our data set that have many reviews.  However, if we choose users with many reviews, it is quite likely that we will miss the \"long tail\" of reviews which will make sure we have high common support. By \"long tail\", we here mean the many users who have rated 1-2 restaurants each, but whose ratings of these restaurants together help put together a reasonable (say, $\\gt$ 10) total of ratings for those restaurants.\n", 
        "\n", 
        "However, we cant choose too few reviews either. Two problems arise when we do this. Firstly, we want to split our data set into train, validate, and test parts. If we have users with too few reviews, we wont have enough data about the user to train on to make any good predictions, given that we must keep some of this user data aside in the test and validation sets. Secondly, if a user has too few reviews, when we look for the $k$ nearest neighbors of the item we are tring to predict on, there will be too many users with a totality of reviews less than $k$.\n", 
        "\n", 
        "But the flip side of this is again, if we create a dataset with users with many reviews, we are creating a more biased sample. Most users review very little.\n", 
        "\n", 
        "The way out of this conundrum is to create validation and test sets with users that have more reviews, so that have data left over. This skews our prediction accuracies towards more prolific users a bit (we take care not to skew it too much), but brings us the ability to have something to validate and test on.\n", 
        "\n", 
        "You should remember that we are doing all this to make a simple small dataset for this homework. Real world datasets need to deal with even greater sparsiy and so on. As an example, see the README for the Netflix Dataset: http://www.cs.utexas.edu/users/downing/netflix/README ."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 6, 
      "cell_type": "code", 
      "source": [
        "def make_smaller(df, bizcount, usercount):\n", 
        "    smallidf1=df[(df.business_review_count > bizcount)]\n", 
        "    smallidf1=recompute_frame(smallidf1)\n", 
        "    smallidf2=smallidf1[(smallidf1.user_review_count > usercount)]\n", 
        "    smalldf=recompute_frame(smallidf2)\n", 
        "    return smalldf"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 7, 
      "cell_type": "code", 
      "source": [
        "smalldf=make_smaller(bigdf, 150, 5)\n", 
        "smalldf.to_csv(\"small.csv\", index=False)\n", 
        "print \"Number of Reviews\",smalldf.shape[0]\n", 
        "print \"Number of Users\", smalldf.user_id.unique().shape[0]\n", 
        "print \"Number of Businesses\", smalldf.business_id.unique().shape[0]\n", 
        "s,d=compute_supports(smalldf)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 8, 
      "cell_type": "code", 
      "source": [
        "plt.hist(s, bins=np.arange(1,50,1), alpha=0.5, label=\"support\");\n", 
        "c=smalldf.groupby('user_id').business_id.count()\n", 
        "print \"User review counts mean and median\",np.mean(c), np.median(c)\n", 
        "plt.hist(c, bins=np.arange(1,50,1), alpha=0.5, label=\"user reviews\");\n", 
        "plt.legend();"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Just in case a user has made a review twice, we drop it."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 9, 
      "cell_type": "code", 
      "source": [
        "print smalldf.shape\n", 
        "smalldf=smalldf.drop_duplicates(['user_id','business_id'])\n", 
        "print smalldf.shape"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### 1.1 Visualize the sparsity of the data set "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Plot two histograms of the review count grouped by the user_id and business_id respectively. The first plot ought to be the number of reviews per user, and the second the number of reviews per restaurant. Describe what you see."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 10, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 11, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "*your answer here*"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Splitting into testing and training sets"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We're going to split our dataset into training, validation, and testing sets. To do this we first group our data by user id. The reason for doing this is that in each of these lists we create, we want each user to be represented. Otherwise, how would we test our predictions for that user? \n", 
        "\n", 
        "Then we choose 5 reviews per user, and put 3 in the validation and 2 in the testing set, as long as that user has rated more than 12 restaurants. All other reviews are left for the training set.\n", 
        "\n", 
        "This implements the strategy we talked about above."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 12, 
      "cell_type": "code", 
      "source": [
        "smalldf[smalldf.user_review_count > 12].shape"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 13, 
      "cell_type": "code", 
      "source": [
        "trainlist=[]\n", 
        "testlist=[]\n", 
        "validatelist=[]\n", 
        "take=5\n", 
        "for k, v in smalldf.groupby('user_id'):\n", 
        "    if np.mean(v.user_review_count) > 12:\n", 
        "        takenos=np.random.choice(range(take), size=take/2, replace=False)\n", 
        "        takelist=np.array([e in takenos for e in range(take)])\n", 
        "        validatelist.append(v[-take:][~takelist])#use those \n", 
        "        testlist.append(v[-take:][takelist])#use the other \n", 
        "        trainlist.append(v[:-take])\n", 
        "    else:\n", 
        "        trainlist.append(v)\n", 
        "traindf=pd.concat(trainlist)\n", 
        "validatedf=pd.concat(validatelist)\n", 
        "testdf=pd.concat(testlist)\n", 
        "print traindf.shape, validatedf.shape, testdf.shape"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "To be on the safe side, we also test that any business from the validation and test sets was encountered in the training set. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 14, 
      "cell_type": "code", 
      "source": [
        "maskval=[e in set(validatedf.business_id).difference(set(traindf.business_id)) for e in validatedf.business_id] \n", 
        "masktest=[e in set(testdf.business_id).difference(set(traindf.business_id)) for e in testdf.business_id] \n", 
        "print np.sum(maskval), np.sum(masktest)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We recompute our sets' counts and averages, and use only some columns on the validation and test sets."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 15, 
      "cell_type": "code", 
      "source": [
        "traindf=recompute_frame(traindf)\n", 
        "validatedf=recompute_frame(validatedf)\n", 
        "testdf=recompute_frame(testdf)\n", 
        "validatedf=validatedf[['user_id', 'business_id','stars', 'review_id']]\n", 
        "testdf=testdf[['user_id', 'business_id', 'stars', 'review_id']]\n", 
        "traindf.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Lets save these files out so that we can use them if we lose our original files."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 16, 
      "cell_type": "code", 
      "source": [
        "traindf.to_csv(\"tempdata/strain.csv\", index=False, header=True, encoding=\"utf-8\")\n", 
        "validatedf.to_csv(\"tempdata/svalidate.csv\", index=False, header=True, encoding=\"utf-8\")\n", 
        "testdf.to_csv(\"tempdata/stest.csv\", index=False, header=True, encoding=\"utf-8\")"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 17, 
      "cell_type": "code", 
      "source": [
        "testdf.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "###The baseline model"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "To motivate our recommendation system, consider the following example. Let's pretend we are in Boston for a second. Lets say the average rating of restaurants here by all the users is 3.5. Sandrine's at Harvard square is better than an average restaurant, so it tends to be rated 0.5 stars above the average of all restaurants. However, you are a curmudgeon, who tends to rate 0.2 stars below the average. Then a baseline estimate for the recommendation for Sandrine's, for you, is 3.5+0.5-0.2=3.8.\n", 
        "\n", 
        "These baseline estimates thus adjust the data by accounting for the systematic tendencies for some users who give higher ratings than others, and for some restaurants to receive higher ratings than others. We can write the baseline estimate $\\hat Y_{um}^{baseline}$ for an unknown rating $Y_{um}$ for user $u$ and restaurant or business $m$ as:\n", 
        "\n", 
        "$$ \\hat Y_{um}^{baseline} = \\hat \\mu + \\hat \\theta_{u} + \\hat \\gamma_{m} $$\n", 
        "\n", 
        "where the unknown parameters $\\theta_{u}$ and $\\gamma_{m}$ indicate the deviations, or biases, of user $u$ and item $m$, respectively, from some intercept parameter $\\mu$. \n", 
        "\n", 
        "Notice that the $\\theta_{u}$ and $\\gamma_{m}$ are parameters which need to be fit. The simplest thing to start with is to replace them by their \"mean\" estimates from the data. Thus:\n", 
        "\n", 
        "$$ \\hat Y^{baseline}_{um} = \\bar Y + (\\bar Y_u - \\bar Y) + (\\bar Y_m - \\bar Y)$$\n", 
        "\n", 
        "where $\\bar Y_u$ =  `user_avg`, the average of all a user $u$'s ratings and $\\bar Y_m$ = `business_avg`, the average of all ratings for a restaurant $m$. $\\bar Y$ is the average rating over all reviews.\n", 
        "\n", 
        "The final two terms correspond to the user-specific and item-specific bias in ratings, that is, how their ratings tend to systematically diverge from the global average. This is the simplest possible way to predict a rating, based only on information about *this* user and *this* restaurant."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We first get the overall mean."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 18, 
      "cell_type": "code", 
      "source": [
        "ybar = traindf.stars.mean()\n", 
        "ybar"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "For book-keeping and ordering we create lists of unique `user_id`s and `business_id`s, and a mapping of these id's to integers which we will use later as indices into similarity databases and regression design matrices"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 19, 
      "cell_type": "code", 
      "source": [
        "uuids=traindf.user_id.unique()#unique-user-ids\n", 
        "uiids=traindf.business_id.unique()#unique-item-ids"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 20, 
      "cell_type": "code", 
      "source": [
        "uuidmap={v:k for k,v in enumerate(uuids)}#of length U\n", 
        "uiidmap={v:k for k,v in enumerate(uiids)}#of length M"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "Then we get the unique userids and restaurant-ids (item ids) and for each such user and item, compute the user's average rating or bias of all the restaurants he/she has rated, and the restaurant's (item's) average rating or bias over all the users who rated it."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### 1.2 Calculate dictionaries of user and restaurant biases and plot histograms for them"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Using the `uuids` and `uiids` create dictionaries `user_biases` and `item_biases` respectively. These have keys user ids and item ids respectively with values user biases and item biases respectively. Plot histograms of both the user bias and item bias. Plot the means and medians as well. Finally, also plot a bar-chart of all the ratings in the training set. Comment on the trends you see. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 21, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 22, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 23, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 24, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "*your answer here*"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We save the mean and the biases into a dictionary for use later"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 25, 
      "cell_type": "code", 
      "source": [
        "train_avgs={'mean':ybar, 'users':user_biases, 'items':item_biases}"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "###Making baseline predictions"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We'll create a dictionary `predictions` with keys the names of the models we use, and value the predictions on the test set. We'll also create a dictionary `predictions_valid` which saves the results of models on the validation set. This might seem a strange thing to do, but as we shall see later, this will come useful in ensembling"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 26, 
      "cell_type": "code", 
      "source": [
        "predictions={}\n", 
        "predictions_valid={}"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "####1.3 Make the prediction from the `baseline` model"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Store the predictions on the test set `testdf` in `predictions[\"baseline\"]`, and on the validation set in `pedictions_valid` with the same key."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 27, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "How did we do?\n", 
        "\n", 
        "We define a function to calculate the root-mean-square error between actual values and predictions. We could have used the corresponding function in `sklearn` as well."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 28, 
      "cell_type": "code", 
      "source": [
        "def get_rmse(s, s_predict):\n", 
        "    diff = s - s_predict\n", 
        "    return np.sqrt(np.dot(diff,diff)/diff.shape[0])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "We now write a `compare_results` function which plots the predicted star values against the actual ones. The x axis is the actual rating, so predicted results at any integer star value will be up and down the grid line at that value. We take the average of the ratings and plot a point there, and fill-inbetween one standard deviation on each side of this point. We also plot the 45 degree line for reference."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 29, 
      "cell_type": "code", 
      "source": [
        "def compare_results(stars_actual, stars_predicted, ylow=1, yhigh=6, model=\"\", predicteds=False, onsame=False, axis=False):\n", 
        "    \"\"\"\n", 
        "    plot predicted results against actual results. Takes 2 arguments: a\n", 
        "    numpy array of actual ratings and a numpy array of predicted ratings\n", 
        "    scatterplots the predictions, a unit slope line, line segments joining the mean,\n", 
        "    and a filled in area of the standard deviations.\"\n", 
        "    \"\"\"\n", 
        "    if onsame:\n", 
        "        ax=onsame\n", 
        "    elif axis:\n", 
        "        ax=axis\n", 
        "    else:\n", 
        "        fig=plt.figure()\n", 
        "        ax=plt.gca()\n", 
        "    df=pd.DataFrame(dict(actual=stars_actual, predicted=stars_predicted))\n", 
        "    xp=[]\n", 
        "    yp=[]\n", 
        "    for k,v in df.groupby('actual'):\n", 
        "        xp.append(k)\n", 
        "        yp.append(v.predicted.mean())        \n", 
        "    \n", 
        "    cl, = ax.plot(xp,yp, 's-', label=\"means for %s\" % model)\n", 
        "    c=cl.get_color()\n", 
        "\n", 
        "    sig=df.groupby('actual').predicted.std().values\n", 
        "    ax.fill_between(xp, yp - sig, yp + sig, \n", 
        "                 color=c, alpha=0.2)\n", 
        "    if predicteds:\n", 
        "        ax.plot(df.actual, df.predicted, '.', color=c, alpha=0.1, label=\"predicted for %s\" % model)\n", 
        "\n", 
        "    if not onsame:\n", 
        "        ax.plot([1,5],[1,5], 'k', label=\"slope 1\")\n", 
        "        ax.set_xlabel(\"actual\")\n", 
        "        ax.set_ylabel(\"predicted\")\n", 
        "        ax.set_ylim([ylow,yhigh])\n", 
        "        ax.set_xlim([0.9, 5.1])\n", 
        "    ax.legend(frameon=False, loc=\"upper left\")\n", 
        "    rmse=get_rmse(stars_actual, stars_predicted)\n", 
        "    print \"RMSE for %s\" % model, rmse\n", 
        "    return ax,rmse"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We see how our prediction works using the baseline model."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 30, 
      "cell_type": "code", 
      "source": [
        "compare_results(testdf.stars,predictions['baseline'], model=\"baseline\", predicteds=True);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "How are we to interpret such diagrams?\n", 
        "\n", 
        "Firstly, note that increasing bias in a model decreases the variation between the rating groups, resulting in a flatter blue curve. Here I am using the term bias in the sense of the bias-variance tradeoff. For example, the most biased model would be to leave out even the user and item \"biases\" (sorry for multiple uses of the term!) and just fit a flat average line, completely horizontal. Our prediction here is less biased than that.\n", 
        "\n", 
        "The second thing to notice is that the precision as measured by the width of the grey region measures the variance WITHIN rating groups, not between them. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "##Q2 Collaborative Filtering using kNN"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "###Setting up the Collaborative Filtering Model"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Can we do a better job of predicting the rating $Y_{um}$ user $u$ would give to restaurant $m$? According to the central dogma of CF, we ought to be able to use the responses of *similar* users regarding *similar* restaurants to get a better prediction. \n", 
        "\n", 
        "We can make an estimate of $Y_{um}$ as:\n", 
        "\n", 
        "$$ \\hat{Y_{um}} = \\hat Y_{um}^{baseline}\\, + \\,\\frac{\\sum\\limits_{j \\in S^{k}(m)} s_{mj} ( Y_{uj} - \\hat Y_{uj}^{baseline} )}{\\sum\\limits_{j \\in S^{k}(m)} s_{mj} } $$\n", 
        "\n", 
        "where $S^{k}(m)$ is the $k$ neighbor items of item $m$ based on some *pooling criterion*, for example, those items which have been rated by user $u$.\n", 
        "\n", 
        "\n", 
        "To do this, we compute a *similarity measure* $s_{mj}$ between the $m$th and $j$th items. This similarity might be measured via [cosine similarity](http://en.wikipedia.org/wiki/Cosine_similarity), [pearson co-efficient](http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) or using other distance based measures. Here we shall use the Pearson coefficient. This measures the tendency of users to rate items similarly. Since most ratings are unknown, it is computed on the \"common user support\" (`n_common`), which is the set of common raters of both items. This is why we spent some time earlier trying to craft a data set which was doable in reasonable time in the context of this homework but still having reasonable values of `n_common`."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "####Global Recommender"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "**$S^{k}(m)$ depends on items only.**\n", 
        "\n", 
        "Now we have a way to pool information between similar restaurants to try to predict a user's recommendation. But how do we choose the neighborhood to pool over? The simplest choice is to calculate the similarity between items using their entire common user support, and rank the nearest neighbors of an item by this similarity. We call this a \"global\" recommender because it assumes that every user perceives the similarity between restaurants in the same way. \n", 
        "The global recommender does have the advantage of dealing with the possible sparsity of the user's rated items, but also the disadvantage of giving one answer for all users, without taking the user's preferences into account. This is a classic case of bias-variance tradeoff."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The above recommender also has a second problem: we have no way of telling how good our results are. The problem is this:\n", 
        "\n", 
        "When I compute the global neighborhood of a restaurant, one of the restaurants in these `k` nearest neighbors might not have a rating from me in the training set, in the usual training-validation paradigm. So I cant compute that $Y_{uj}$ on the right hand side in the formula above. The best I could do is to use the average rating of restaurant in the training set. I can, of-course, still calculate the nearest neighbors and make recommendations based on the distances I calculate. \n", 
        "\n", 
        "We shall not do this recommender here. See HW4 in the 2013 cycle of cs109 for an example of such a recommender. But, you might see such a recommender in the wild: its then a good faith recommender, as it is very hard to validate such a recommender without playing games with averages."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "####Local recommender"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "**$S^{k}(m) = S^{k}(m;u)$ depends on items and the user in question **\n", 
        "\n", 
        "The aim is to get more fine-grained predictions about each user, and try to predict what _rating_ a user would give to a restaurant they have never tried before. To do this, we will try to personalize the information we use even further, and only pool information from restaurants that the user has rated.\n", 
        "\n", 
        "This allows us to return to the original problem of prediction $Y_{um}$ for a restaurant $m$ that user $u$ has never rated before. Using our newly computed similarity metrics, we can modify our original baseline estimate by pulling in information from the user's neighborhood of the restaurant $m$, and predict $Y_{um}$ as:\n", 
        "\n", 
        "$$ \\hat{Y_{um}} = \\hat Y^{baseline}_{um}\\, + \\,\\frac{\\sum\\limits_{j \\in S^{k}(m;u)} s_{mj} ( Y_{uj} - \\hat Y^{baseline}_{uj} )}{\\sum\\limits_{j \\in S^{k}(m;u)} s_{mj} } $$\n", 
        "\n", 
        "where $s^{k}(m;u)$ is the $k$ neighbor items of item $m$ which have been rated by user $u$.\n", 
        "\n", 
        "Now, this is not a particularly good assumption, especially in the situation where a restaurant is new (new item problem) or a user is new (cold start problem), or in the case when there are very few reviewers of a restaurant, or very few reviews by a user respectively. However, one must start somewhere!\n", 
        "\n", 
        "Notice that in adding in the similarity term, we subtract the baseline estimate from the observed rating of the user's neighbor items."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "###Common Support"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Lets now make a histogram of the common user support (the number of common reviewers) of each pair of restaurants on the training set, and print the mean. We saw a function do this earlier when we created the smaller data set in `smalldf`. \n", 
        "\n", 
        "The common support is an important concept, as for each pair of restaurants, it is the number of people who reviewed both. It will be used to modify similarity between restaurants. If the common support is low, the similarity is less believable, and we would want to \"regularize\" it by some average estimate.\n", 
        "\n", 
        "In this incarnation we create a two dimensional array `supports` which uses the `uiidmap`, and a Pandas Series `users_for_restaurants` with index the restaurant ids which will come useful later. We obtain the common reviewers as an intersection between two restaurants reviewers. We take care to fill the entire matrix including the diagonal. (This is the second implementation of a support finding routine you have seen: we use set intersections here; earlier we used boolean ANDs on a mask). You will write a third one, soon :-)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 31, 
      "cell_type": "code", 
      "source": [
        "#for each restaurant get the unique userids who rated it\n", 
        "users_for_restaurants = traindf.groupby('business_id').user_id.unique()\n", 
        "restaurants=uiids\n", 
        "lres=len(restaurants)\n", 
        "supports=[[[] for i in range(lres)] for i in range(lres)]\n", 
        "supporthistlist=[]\n", 
        "for i,rest1 in enumerate(restaurants):\n", 
        "    for j,rest2 in enumerate(restaurants):\n", 
        "        if  i <= j:#its symmetric\n", 
        "            if rest1==rest2:\n", 
        "                common_reviewers=users_for_restaurants[rest1]\n", 
        "            else:\n", 
        "                common_reviewers = set(users_for_restaurants[rest1]).intersection(set(users_for_restaurants[rest2]))\n", 
        "                supporthistlist.append(len(common_reviewers))\n", 
        "            supports[i][j]=common_reviewers\n", 
        "            supports[j][i]=common_reviewers\n", 
        "print \"Mean and Median support is:\",np.mean(supporthistlist), np.median(supporthistlist)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We see that the common support is a bit lower on the training set due to the loss of some ratings to the validation and test sets."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "###Calculating similarity"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Remember that even though each reviewer uses the same 5-star scale when rating restaurants, comparing two users by comparing their raw user ratings can be problematic. Consider a user whose average rating is 2. This is a curmudgeonly user. Consider another whose average rating is 4. This is a rather enthusiastic one. How should we compare a 3 rating by the curmudgeonly one to a 5 rating of the enthusiastic one? This is why we computed the baseline model in the last problem."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### 2.1 Write a function `pearson_sim` that calculates the pearson similarity"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We must subtract the average rating of the user from the actual rating of the restaurants in computing the similarity of two restaurants. This makes the ratings by the two users comparable. We do this in the function `pearson_sim` defined below.\n", 
        "\n", 
        "If there is no common support (`n_common=0`), we have no basis for making a similarity estimate, and so we set the similarity to 0. In the case that the individual restaurant rating variance is 0, such as in the case where there is only one common reviewer (`n_common=1`), convert the `NaN` that the scipy `pearsonr` returns to`0.`."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 32, 
      "cell_type": "code", 
      "source": [
        "from scipy.stats.stats import pearsonr\n", 
        "\"\"\"\n", 
        "Given a subframe or dictionary of restaurant 1 reviews and a subframe or dictionary of restaurant 2 reviews,\n", 
        "where the reviewers are those who have reviewed both restaurants, return \n", 
        "the pearson correlation coefficient between the user average subtracted ratings.\n", 
        "The case for zero common reviewers is handled separately. If the correlation is\n", 
        "NaN if any of the individual variances are 0 (the n=1 case), return 0 instead\n", 
        "\"\"\"\n", 
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "####Get Restaurant reviews"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The function `get_restaurant_reviews` defined below takes a restaurant `business_id` and a set of users, and returns the reviews of that restaurant by those users. You will use this function in calculating a similarity function below. The set of users is all the users that rated a restaurant for the global recommender, and later, a restricted set for the local recommender."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 33, 
      "cell_type": "code", 
      "source": [
        "def get_restaurant_reviews(restaurant_id, df, set_of_users):\n", 
        "    \"\"\"\n", 
        "    given a resturant id and a set of reviewers, return the sub-dataframe of their\n", 
        "    reviews.\n", 
        "    \"\"\"\n", 
        "    mask = (df.user_id.isin(set_of_users)) & (df.business_id==restaurant_id)\n", 
        "    reviews = df[mask]\n", 
        "    reviews = reviews[reviews.user_id.duplicated()==False]\n", 
        "    return reviews"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "### Making a database of similarities"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We now move to calculating a global database of pairwise restaurant similarities.\n", 
        "We provide you here with a Class to make a database of the similarities for each pair of restaurants in the database. \n", 
        "\n", 
        "The class `Database` is initialized in its constructor `__init__` . A `class` is a construct which lets you store state in a program. Here we create a array for the database and fill it with zeros. \n", 
        "\n", 
        "The `set_supports` method is used to store the `supports` that we computed earlier.\n", 
        "\n", 
        "The `get` method on the database can be used to retrieve the similarity for two business ids.\n", 
        "\n", 
        "The general strategy for filling in this database is as follows:\n", 
        "\n", 
        "1. For each of the two restaurants, we get the set of reviewers who have reviewed the restaurant and compute the intersection of these two sets. We also compute the number of common reviewers `n_common`.\n", 
        "\n", 
        "2. We get the reviews for each restaurant as made by these common reviewers and get the star ratings and user averages. \n", 
        "\n", 
        "3. We calculate the similarity using `similarity_func`.\n", 
        "\n", 
        "4. We return the similarity and `n_common` in a tuple `(sim, n_common)`. If the similarity is a `NaN`, set the similarity to 0."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 34, 
      "cell_type": "code", 
      "source": [
        "class Database:\n", 
        "    \"A class representing a database of similarities and common supports\"\n", 
        "    \n", 
        "    def __init__(self, rindexmap, supports):\n", 
        "        \"the constructor, takes a map of restaurant id's to integers\"\n", 
        "        database={}\n", 
        "        self.rindexmap=rindexmap\n", 
        "        self.supports=supports\n", 
        "        l_keys=len(self.rindexmap.keys())\n", 
        "        self.database_sim=np.zeros([l_keys,l_keys])\n", 
        "        self.database_sup=np.zeros([l_keys, l_keys], dtype=np.int)\n", 
        "\n", 
        "    def set_supports(self, supports):\n", 
        "        self.supports=supports\n", 
        "        \n", 
        "    def get(self, b1, b2):\n", 
        "        \"returns a tuple of similarity,common_support given two business ids\"\n", 
        "        sim=self.database_sim[self.rindexmap[b1]][self.rindexmap[b2]]\n", 
        "        nsup=self.database_sup[self.rindexmap[b1]][self.rindexmap[b2]]\n", 
        "        return (sim, nsup)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "Lets create an instance of the `Database class`, using the `supports` and `uiidmap` we calculated when making the histogram of common supports and even earlier"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 35, 
      "cell_type": "code", 
      "source": [
        "db=Database(uiidmap, supports)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "### Map reduce to populate database"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We shall fill in the database using a map-reduce computation, which can, if implemented in mrjob or Spark, be scaled over multiple machines. Here we just wish to illustrate this process, and right our own implementation.\n", 
        "\n", 
        "We display here, an outline of the implementation, so that the steps in the process can be followed:\n", 
        "\n", 
        "```python\n", 
        "def map_reduce(tuples)\n", 
        "    mapped1=map(mapper1, tuples)\n", 
        "    combine1=combiner(mapped1)\n", 
        "    reduced1=reduce(lambda x,y: x + [reducer1(y)], combine1, [])\n", 
        "    mapped2=map(mapper2,reduced1)\n", 
        "    combine2=combiner_list(mapped22)\n", 
        "    output=reduce(lambda x,y: x + [reducer2(y)], combine2, [])\n", 
        "    return output\n", 
        "tuples=traindf.itertuples()\n", 
        "sims=map_reduce(tuples)\n", 
        "```"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 36, 
      "cell_type": "code", 
      "source": [
        "print traindf.columns\n", 
        "for row in traindf.head(2).itertuples():\n", 
        "    print row"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Note the additional index at the beginning of the tuple.\n", 
        "\n", 
        "**(1)** In `mapper1`, we first take *each* tuple in the form above from the dataframe, and apply the function using python's `map` to spit out a tuple of the form: `user_id, (business_id, stars, user_avg)`. `mapped1` is then a list of these tuples.\n", 
        "\n", 
        "`mapped1=map(mapper1, tuples)`"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 37, 
      "cell_type": "code", 
      "source": [
        "def mapper1(row):\n", 
        "    return row[1], (row[2], row[5], row[14])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "**(2)** Then, in `combiner` we do what a map reduce framework like hadoop does for us and \"collect\" all the tuples belonging to a given `user_id` to form a list `combine1` of the form `[(user_id, list of (business_id, stars, user_avg) ),...]`\n", 
        "\n", 
        "`combine1=combiner(mapped1)`"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 38, 
      "cell_type": "code", 
      "source": [
        "def combiner(items):\n", 
        "    indict={}\n", 
        "    for key, value in items:\n", 
        "        if not indict.has_key(key):\n", 
        "            indict[key]=[]\n", 
        "        indict[key].append(value)\n", 
        "    return indict.items()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "**(3)** Then, in `reducer1`, we use python's `reduce` function to reduce the list we made above into another list `reduced1`. This list is made by starting from the empty list `x=[]`. Then for each element `y` in `combine1`, we return a tuple of `user_id` with a list of tuples `[(business_id,(stars, user_avg)),...]`. Thus `reduced1` is a list of tuples `[(user_id, [(business_id,(stars, user_avg)),...]),...]`.\n", 
        "\n", 
        "`reduced1=reduce(lambda x,y: x + [reducer1(y)], combine1, [])`"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 39, 
      "cell_type": "code", 
      "source": [
        "def reducer1(the_input):\n", 
        "    user_id, values = the_input\n", 
        "    ratings=[]\n", 
        "    for business_id,stars,user_avg in values:\n", 
        "        ratings.append((business_id,(stars, user_avg)))\n", 
        "    return user_id, ratings"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "**(4)** Its now your job to write the next mapper `mapper2`:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### 2.2 Write `mapper2`"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "`mapped2=map(mapper2,reduced1)`\n", 
        "\n", 
        "`mapper2` takes each element of the list `reduced1`, that is, something that looks like `(user_id, [(business_id,(stars, user_avg)),...])`. Use the function `combinations_with_replacement` from python's itertools module, and comes up with all possible 2-combinations of elements from the list `[(business_id,(stars, user_avg)),...]` (including those which repeat the element. You will notice that what this does is to output every combination of restaurant_id's that the user in question has reviewed.\n", 
        "\n", 
        "So, ignoring the user_id key, take all combinations of business pairs and return a list whose items are tuples. Each tuple has 2 elements. The first element is a tuple of pair ids, and the second tuple is a tuple of pair rating information. The first tuple looks something like `(biz1_id, biz2_id)`, while the second one looks like `((stars1, user_avg1),(stars2, user_avg2))`. Make sure that if `biz1id > biz2id` lexicographically (which is a big way of saying alphabetically: using the alphabets imposes an ordering on restaurant ids), you reverse the order of both the tuples. What this does is that it ensures that the first business id is always smaller than or equal to the second one and thus we dont do extra work."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 40, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "**(5)** We now run `mapper2` on the entire list `reduced1` and as before, run a combiner to *collect* all tuple-pairs with the first tuple as the key. This will now give us, for each business id pair, a list of `((stars1, user_avg1),(stars2, user_avg2))` tuples, and thus, a list of the common support of each business id pair, in the form `[(biztuple, list of ((stars1, user_avg1),(stars2, user_avg2))),...]` which we now store in `combine2`.\n", 
        "\n", 
        "`combine2=combiner_list(mapped22)`\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 41, 
      "cell_type": "code", 
      "source": [
        "def combiner_list(itemslist):\n", 
        "    indict={}\n", 
        "    for items in itemslist:\n", 
        "        for key, value in items:\n", 
        "            if not indict.has_key(key):\n", 
        "                indict[key]=[]\n", 
        "            indict[key].append(value)\n", 
        "    return indict.items()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "**(6)** Now we write the next reducer `reducer2`."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### 2.3 Write `reducer2`"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "From `combiner_list`, for each business_id pair we get `common_ratings` which is a list of tuples with ratings by the same user for the two businesses in the form `[((stars1, user_avg1),(stars2, user_avg2)),..]`. Write a function `reducer2` that takes the output of `combiner_list` and uses these common ratings to return, for each restaurant pair, two tuples. The first tuple if a tuple of restaurant ids, while the second tuple is their pearson correlation and their common support. In other words, the last line of this reducer function ought to look something like `return (restaurant1_id, restaurant2_id), (rho, n_common)`.\n", 
        "Given that the output \n", 
        "\n", 
        "HINT: By converting these to dictionaries with values that are `np.array`s we can use the same `pearson_sim` similarity function we defined above."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 42, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "Ok, so we create the `map_reduce` function for real and then run it!"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 43, 
      "cell_type": "code", 
      "source": [
        "def map_reduce(tuples):\n", 
        "    mapped1=map(mapper1, tuples)\n", 
        "    combine1=combiner(mapped1)\n", 
        "    reduced1=reduce(lambda x,y: x + [reducer1(y)], combine1, [])\n", 
        "    mapped2=map(mapper2,reduced1)\n", 
        "    combine2=combiner_list(mapped2)\n", 
        "    output=reduce(lambda x,y: x + [reducer2(y)], combine2, [])\n", 
        "    return output"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 44, 
      "cell_type": "code", 
      "source": [
        "tuples=traindf.itertuples()\n", 
        "sims=map_reduce(tuples)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We now populate our instance of the database class by passing the instance `db` to the function `populate_from_mr` below. It takes the output of the reducer, finds the integer indices corresponding to the business id's, and populates the slots in the database."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 47, 
      "cell_type": "code", 
      "source": [
        "def populate_from_mr(db, df, mapredlist):\n", 
        "    for tpair,vpair in mapredlist:\n", 
        "        i1=db.rindexmap[tpair[0]]\n", 
        "        i2=db.rindexmap[tpair[1]]\n", 
        "        db.database_sim[i1][i2]=vpair[0]\n", 
        "        db.database_sup[i1][i2]=vpair[1]\n", 
        "        db.database_sim[i2][i1]=vpair[0]\n", 
        "        db.database_sup[i2][i1]=vpair[1]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 48, 
      "cell_type": "code", 
      "source": [
        "populate_from_mr(db, traindf, sims)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### Checking our work"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "To check your work, since this is your first exposure to map-reduce, we provide you functions to do this the usual way using numpy and pandas. We first create a second instance of the database."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 49, 
      "cell_type": "code", 
      "source": [
        "db2=Database( uiidmap, supports)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "The function `populate_by calculating` iterates over every possible pair of business_id's in the dataframe and populates the database with similarities and common supports. It takes as arguments a function the similarity function `similarity_func` like `pearson_sim` (`calculate_similarity` uses this to calculate the similarity).\n", 
        "\n", 
        "We write the function `calculate_similarity` inside this class that is the workhorse method that we use to populate the database. This method operates between two restaurants and calculates a similarity for them, taking a dataframe and a similarity function `similarity_func`. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 50, 
      "cell_type": "code", 
      "source": [
        "def calculate_similarity(db, df, rest1, rest2, similarity_func):\n", 
        "    # find common reviewers\n", 
        "    common_reviewers = db.supports[db.rindexmap[rest1]][db.rindexmap[rest2]]\n", 
        "    n_common=len(common_reviewers)\n", 
        "    if rest1==rest2:\n", 
        "        return 1., n_common\n", 
        "    #get reviews\n", 
        "    rest1_reviews = get_restaurant_reviews(rest1, df, common_reviewers)\n", 
        "    rest2_reviews = get_restaurant_reviews(rest2, df, common_reviewers)\n", 
        "    sim=similarity_func(rest1_reviews, rest2_reviews, n_common)\n", 
        "    return sim, n_common\n", 
        "\n", 
        "def populate_by_calculating(db, df, similarity_func):\n", 
        "    \"\"\"\n", 
        "    a populator for every pair of businesses in df. takes similarity_func like\n", 
        "    pearson_sim as argument\n", 
        "    \"\"\"\n", 
        "    items=db.rindexmap.items()\n", 
        "    for b1, i1 in items:\n", 
        "        for b2, i2 in items:\n", 
        "            if i1 <= i2:\n", 
        "                sim, nsup=calculate_similarity(db, df, b1, b2, similarity_func)\n", 
        "                db.database_sim[i1][i2]=sim\n", 
        "                db.database_sim[i2][i1]=sim\n", 
        "                db.database_sup[i1][i2]=nsup\n", 
        "                db.database_sup[i2][i1]=nsup"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "We run the `populate_by_calculating` function."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 51, 
      "cell_type": "code", 
      "source": [
        "%%time\n", 
        "populate_by_calculating(db2, traindf, pearson_sim)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Lets check it for a pair of results. Try some other tests. *These specific two may not be there in the training set so you will need to find your own test businesses.*"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 52, 
      "cell_type": "code", 
      "source": [
        "tpair=('FV0BkoGOd3Yu_eJnXY15ZA', 'O-Xa9GCFWI65YiBD5Jw_hA')\n", 
        "print db.get(tpair[0],tpair[1]),db2.get(tpair[0],tpair[1])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 53, 
      "cell_type": "code", 
      "source": [
        "print db.get(tpair[1],tpair[0]),db2.get(tpair[1],tpair[0])\n", 
        "print db.get(tpair[0],tpair[0]),db2.get(tpair[0],tpair[0])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### Shrunk similarities"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We are now going to find the k-nearest restaurants to a given restaurant based on the database of similarities that we calculated. But we have a problem.\n", 
        "\n", 
        "Consider the two cases where there are two common reviewers, and where there are 40. In the former case, we might get a artificially high similarity based on the tastes of just this user, and thus we must reduce its importance in the nearest-neighbor calculation. In the latter case, we would get a much more unbiased estimator of the similarity of the two restaurants.\n", 
        "\n", 
        "To control the effect of small common supports, we can **shrink** our pearson co-efficients. We shall do this by using the \"regularization\" parameter `reg`:\n", 
        "\n", 
        "$$s_{mj} = \\frac{N_{common}\\, \\rho_{mj}}{N_{common}+reg} $$\n", 
        "\n", 
        "where $N_{common}$ (`n_common`) is the common reviewer support and $\\rho_{ij}$ is the pearson co-relation coefficient."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Recall the notions of regularization introduced in class. We want to reduce the variance in our estimates, so we pull our estimates in toward a conservative point in a way that strongly corrals in estimates when there is very little data, but allows the data to speak when there is a lot. \n", 
        "\n", 
        "We define a function `shrunk_sim` which takes a `sim` and `n_common`, and shrinks the similarity down using the regularizer `reg`."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 54, 
      "cell_type": "code", 
      "source": [
        "def shrunk_sim(sim, n_common, reg=3.):\n", 
        "    \"takes a similarity and shrinks it down by using the regularizer\"\n", 
        "    ssim=(n_common*sim)/(n_common+reg)\n", 
        "    return ssim"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "###Calculating Nearest Neighbors"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Now we can move to writing a `knearest` function, which finds the `k` nearest neighbors of a given restaurant based on the shrunk similarities we calculate. Note that as defined here, the nearest neighbors are restricted to the restaurants a user has reviewed."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "####Similarity Distances"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The very idea of nearest neighbor should engender in you the idea of distances; and distances should not be negative.\n", 
        "Furthermore, you would not be able to use distances other than those implied by the correlation coefficient in the calculation of nearest neighbors (such as a Manhattan distance or Jacard similarity).\n", 
        "\n", 
        "We can fix this for the case of pearson coefficient by just a simple rescaling to the 0-1 range!\n", 
        "\n", 
        "$$ \\rho \\rightarrow \\frac{1 - \\rho}{2} $$.\n", 
        "\n", 
        "Notice this quantity is always positive and between 0 and 1. Its 0 at full correlation, and 1 at full anticorrelation."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### We write a function `knearest` "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "This function returns a *k-length sorted list* of 3-tuples each corresponding to a restaurant, the k-nearest restaurants to one we ask for.\n", 
        "\n", 
        "The tuple structure is `(business_id, shrunken distance, common support)` where the distance and common support are with respect to the restaurant whose neighbors we are finding, and the `business_id` is the id of the \"nearby\" restaurant found. The nearby restaurants are found from a supplied numpy array of restaurants `set_of_restaurants`. We use `itemgetter` from the `operator` module to do the sorting. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 55, 
      "cell_type": "code", 
      "source": [
        "\"\"\"\n", 
        "Function\n", 
        "--------\n", 
        "knearest\n", 
        "\n", 
        "Parameters\n", 
        "----------\n", 
        "restaurant_id : string\n", 
        "    The id of the restaurant whose nearest neighbors we want\n", 
        "set_of_restaurants : array\n", 
        "    The set of restaurants from which we want to find the nearest neighbors\n", 
        "dbase : instance of Database class.\n", 
        "    A database of similarities, on which the get method can be used to get the similarity\n", 
        "  of two businesses. e.g. dbase.get(rid1,rid2)\n", 
        "k : int\n", 
        "    the number of nearest neighbors desired, default 7\n", 
        "reg: float\n", 
        "    the regularization.\n", 
        "    \n", 
        "  \n", 
        "Returns\n", 
        "--------\n", 
        "A sorted list\n", 
        "    of the top k similar restaurants. The list is a list of tuples\n", 
        "    (business_id, shrunken similarity, common support).\n", 
        "\"\"\"\n", 
        "from operator import itemgetter\n", 
        "def knearest(restaurant_id, set_of_restaurants, dbase, k=7, reg=3.):\n", 
        "    \"\"\"\n", 
        "    Given a restaurant_id, dataframe, and database, get a sorted list of the\n", 
        "    k most similar restaurants from the set of restaurants.\n", 
        "    \"\"\"\n", 
        "    similars=[]\n", 
        "    for other_rest_id in set_of_restaurants:\n", 
        "        if other_rest_id!=restaurant_id:\n", 
        "            sim, nc=dbase.get(restaurant_id, other_rest_id)\n", 
        "            ssim=shrunk_sim(sim, nc, reg=reg)\n", 
        "            simdist=(1. - ssim)/2.\n", 
        "            similars.append((other_rest_id, simdist, nc ))\n", 
        "    similars=sorted(similars, key=itemgetter(1))\n", 
        "    return similars[0:k]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "We write a function `get_users_restaurants` to get the restaurants a user has rated, and what rating he or she gave them, in preparation for the prediction step."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 56, 
      "cell_type": "code", 
      "source": [
        "def get_users_restaurants(df, user_id):\n", 
        "    dfuser=df[df.user_id==user_id]\n", 
        "    dfuserdedup=dfuser.drop_duplicates('business_id')\n", 
        "    return dict(zip(dfuserdedup.business_id.values, dfuserdedup.stars.values))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "#### 2.4 Write a function to calculate the predicted rating for a user and restaurant"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Now write a function that returns the predicted rating for a user and an item using the formula at the beginning of this problem. Include code to deal with the possibility that the sum of scores that goes in the denominator is 0: return an predicted rating of the baseline portion of the formula in that case. This function `rating` takes as arguments the `train_map` (for e.g., see `train_avg` from earlier which stores the baseline estimates of intercept (overall mean), user bias, and item bias), the database, the `set_of_restaurants`, the wanted `restaurant_id` and `user_id`, and `k` as well as the regularizer `reg`."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 57, 
      "cell_type": "code", 
      "source": [
        "\"\"\"\n", 
        "Function\n", 
        "--------\n", 
        "rating\n", 
        "\n", 
        "Parameters\n", 
        "----------\n", 
        "set_of_restaurants: Dictionary\n", 
        "    The dictionary of restaurant: star-rating pairs you want to make the prediction from.\n", 
        "    This would be the output of a function like get_users_restaurants\n", 
        "train_map: Dictionary\n", 
        "    A dictionary with keys mean, users and items which have estimates of\n", 
        "    overall average or intercept, user coefficients(averages), and\n", 
        "    item coefficients(averages) respectively\n", 
        "dbase : instance of Database class.\n", 
        "    A database of similarities, on which the get method can be used to get the similarity\n", 
        "  of two businessed. e.g. dbase.get(rid1,rid2)\n", 
        "restaurant_id : string\n", 
        "    The id of the restaurant whose nearest neighbors we want\n", 
        "user_id : string\n", 
        "    The id of the user, in whose reviewed restaurants we want to find the neighbors\n", 
        "k : int\n", 
        "    the number of nearest neighbors desired, default 7\n", 
        "reg: float\n", 
        "    the regularization.\n", 
        "    \n", 
        "  \n", 
        "Returns\n", 
        "--------\n", 
        "A float\n", 
        "    which is the imputed rating that we predict that user_id will make for restaurant_id\n", 
        "    \n", 
        "Notes\n", 
        "--------\n", 
        "If the sum of scores is 0, return the baseline estimate of the ranking.\n", 
        "\"\"\"\n", 
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "###Predicting on a user's neighborhood"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "####An example"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Here is an example of how the prediction works:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 58, 
      "cell_type": "code", 
      "source": [
        "trainuser=traindf.loc[0].user_id\n", 
        "testrest=testdf[testdf.user_id==trainuser].business_id.values[0]\n", 
        "print trainuser, testrest"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 59, 
      "cell_type": "code", 
      "source": [
        "def get_actual(df, userid, bizid):\n", 
        "    return df[(df.user_id==userid) & (df.business_id==bizid)]['stars'].values[0]\n", 
        "\n", 
        "print \"Actual\", get_actual(testdf, trainuser, testrest)\n", 
        "print \"Predicted\",rating(get_users_restaurants(traindf, trainuser), train_avgs, db, testrest, trainuser, k=2, reg=3.)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We now write a function which uses the rating function you defined to make a set of predictions on the validation set. Note the default values for the `k` nearest neighbors and regularization `reg`. This function takes an input dataframe `indf`, typically the validation or test dataframe, and iterated over the ratings in this frame, using the `rating` function you wrote to make predictions. It returns a tuple `preds, actuals`. Each of the variables represents an array of ratings from `indf`, with `preds` being the predictions we made and `actuals` being the actual ratings."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 60, 
      "cell_type": "code", 
      "source": [
        "def get_ratings_user_nbd(indf, traindf, train_map, db, k=2, reg=3.):\n", 
        "    zips=zip(indf.business_id, indf.user_id, indf.stars)\n", 
        "    preds=[]\n", 
        "    actuals=[]\n", 
        "    for (r,u,actual) in zips:\n", 
        "        pred=rating(get_users_restaurants(traindf, u),train_map, db, r,u, k, reg)\n", 
        "        preds.append(pred)\n", 
        "        actuals.append(actual)\n", 
        "    return np.array(preds), np.array(actuals)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "To show the usage of this function, lets set $k$ and `reg` to 4 and use it on the training set itself so that we can see what overfitting looks like. Since the training data frame is large, this might take a minute to run."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 61, 
      "cell_type": "code", 
      "source": [
        "pt, at = get_ratings_user_nbd(traindf, traindf, train_avgs, db, k=4, reg=4.)\n", 
        "compare_results(at,pt, model=\"knn(user) on training k=4, reg=4\", predicteds=True)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Note the really low RMSE."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### 2.5 Plot graphs for a 2x2 grid of `k` and `reg` and interpret your results"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Run four different models, for the combinations of parameters from `k=[2,15]` and `reg=[1.,15.]`. For each model, use `compare_results` to plot a graph of the results. Use the `axis` keyword argument of `compare_results` to plot these on a 2x2 grid. Interpret your results from both the rmse of the models and the graphs."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 62, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "*your answer here*"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "####2.6 Validate over `k` and `reg`"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Now use this function defined in a Validation loop over different values of `k` between 1 and 100 and `reg` between 1 and 100. (Note: this is validation, not cross-validation). The validation loop will take a few minutes to run. Start with a coarse loop, but then fine grain it down to be within 5 on both parameters. Store in a dictionary `rmsedict`, keyed by the tuple `(k, reg)` the values of the root mean square error between the predictions and the actual values.\n", 
        "\n", 
        "Depending on how many points you have on your gris, this might take a few minutes to run."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 63, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Note the general flatness of the minimization surface in the parameter space.\n", 
        "\n", 
        "We now get the values of the `k` and `reg` which minimize the root mean square error."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 64, 
      "cell_type": "code", 
      "source": [
        "mintup=min(rmsedict, key=rmsedict.get)\n", 
        "mintup"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "####2.7 Use `mintup` to calculate and plot the results on the test set and compare the results with the baseline model"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Make predictions on the test and validation set. Store these in `predictions['knn']` and `predictions_valid['knn']` respectively. Plot the results on the test set and explain them. Compare the results against the  baseline model. Explain your results and how you think it will change with test and validation sets having reviews of restaurants with more support and users with more rankings."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 65, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "*your answer here*"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "###How to interpret these results?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "In other words, how would you tell a user where to go eat?\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 66, 
      "cell_type": "code", 
      "source": [
        "randomuser=np.random.choice(uuids)\n", 
        "randomrest=np.random.choice(uiids)\n", 
        "\n", 
        "set_of_restaurants = get_users_restaurants(traindf, randomuser)\n", 
        "tops=knearest(randomrest, set_of_restaurants, db, k=mintup[0], reg=mintup[1])\n", 
        "print \"For restaurant\",randomrest, \" and user\", randomuser, \", top matches are:\"\n", 
        "for i, (biz_id, sim, nc) in enumerate(tops):\n", 
        "    print i,biz_id, \"| Sim\", sim, \"| Support\",nc,\"| Actual\", get_actual(traindf, randomuser, biz_id)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "One can also find the top recommendations for a user, for example. See 2013 homework for this."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Q3: The Baseline model with regularization"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The model we used in the last section was:\n", 
        "\n", 
        "$$ \\hat Y_{um}^{baseline} = \\hat \\mu + \\hat \\theta_{u} + \\hat \\gamma_{m} $$\n", 
        "\n", 
        "where we estimated $\\hat \\theta_{u}$ as $\\bar Y_u - \\bar Y$ and $\\hat \\gamma_{m}$ as $\\bar Y_m - \\bar Y$."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "###Writing it as a Ridge Regression"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "In this section we'll get more sophisticated and estimate these quantities as regularized regressions.\n", 
        "\n", 
        "Look at the formula above. Notice that there is a constant term, $\\hat \\mu$, which looks like an intercept, and $N_{user}=U$ plus $N_{item}=M$ terms that dont exactly look like coefficients of a regression.\n", 
        "\n", 
        "There are two key observations to be made which help us cast this problem as a regression. The first one is that we can flatten the non-zero elements of the recommendation matrix $ \\hat Y_{um}^{baseline}$ into a column vector: \n", 
        "\n", 
        "![representation](representation.png)\n", 
        "The second key observation to make to recast the above equation into the following form:\n", 
        "\n", 
        "$$ Y_{um}^{baseline} =  \\mu + \\bar \\theta \\cdot I_{u} +  \\bar \\gamma \\cdot I_{m} $$\n", 
        "\n", 
        "where $I_{u}$ and $I_{m}$ are indicator variables for the uth user and mth item that go into the feature matrix. Here $\\bar \\theta$ means the vector of all thetas, ie a vector of all the coefficients for users who have made ratings in our training set. Similarly $\\bar \\gamma$ is a vector of coefficients for all the items for which ratings have been made in our training set. In our previous model these coefficients were simply the sample means we used.\n", 
        "\n", 
        "The indicator $I_{u}$ is a feature vector that takes on value 1 if it is user $u$ and 0 otherwise. We stack this horizontally with the indicator for item $m$ rated to create a row of the feature matrix. In other words, the row of our feature matrix for the regression for  $Y_{um}^{baseline} $ looks like\n", 
        "\n", 
        "`1 (intercept),0(user 1),0(user 2),..,1(user u),..0(user N_users),0(item 1),0(item 2),..,1(item m),..,0(item N_items)`\n", 
        "\n", 
        "This can be seen in the diagram below.\n", 
        "The left hand red column vector is the response, or the flattened-to-vector recommendation matrix. Note that now we are flattening only the known responses, and among then, even, only those on the training set. The right hand side has the intercept term (which used to be the baseline mean) plus the feature matrix multiplied by the co-efficients vector.  \n", 
        "\n", 
        "And each row of this matrix consists of the horizontally stacked Indicator row vectors.  For example, if we are talking about the user with index 3 and item with index (ie, for $Y^{baseline}_{35}$), the first indicator vector has a 1 at the third (index 2) position and the second indicator has a 1 at the 5th (index 4) position.Then we put these side by side to create the entire row, moving on to do the same thing for the entire next row.\n", 
        "\n", 
        "![expanding](expanding.png)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Now we are in business and can do a simple regression. \n", 
        "\n", 
        "\n", 
        "BUT.\n", 
        "\n", 
        "Notice that we have $U + M + 1$ coefficients to fit. Thats a lot. And in a problem that is very sparse..most of the actual observations of rating of a restaurant by a user simply dont exist. Users rate only a few restaurants each. All of this should remind you of the dreaded word of Machine Learning: Overfitting.\n", 
        "\n", 
        "But we know how to deal with overfitting. We use Regularization. \n", 
        "\n", 
        "Thus we modify our model to:\n", 
        "\n", 
        "$$  Y_{um}^{baseline} =  \\mu +  \\bar \\theta \\cdot I_{u} +  \\bar \\gamma \\cdot I_{m} $$\n", 
        "\n", 
        "such that we minimize:\n", 
        "\n", 
        "$$\\sum_{u,m} \\left( Y_{um} -  \\mu - \\bar \\theta \\cdot I_{u} -  \\bar \\gamma \\cdot I_{m} \\right)^2 + \\alpha \\left( \\theta_{u}^2 + \\gamma_{m}^2 \\right)$$\n", 
        "\n", 
        "Now, this is exactly the form of a ridge regression on the feature matrix with horizontally stacked indicator vectors!\n", 
        "\n", 
        "So let us set up the feature matrix (also called design matrix) for a ridge regression. In `sklearn` we dont need to model the intercept in the design matrix so the size of the design matrix is the number of samples in the training set times the sum of unique users and unique items."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 67, 
      "cell_type": "code", 
      "source": [
        "from sklearn.linear_model import Ridge\n", 
        "features=np.concatenate([uuids,uiids])\n", 
        "features.shape"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "To construct the matrix, we take our dataframe, and set up a matrix of size $n_{samples} \\times n_{features}$ and fill it with zero. We also set up the response $Y_{um}$. We then put a 1 in the design matrix for the $u$ and $m$ corresponding to the specific rating (thus two 1's in each sample row, all others 0's)."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### 3.1 Write a function to produce the design matrix"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Write a function `getmats` which takes an input dataframe `indf` and outputs a tuple `designmatrix, ratings`. The `ratings` are the ratings from the `stars` column of the dataframe, in the order they were present in the dataframe. The design matrix is as described above, keeping this order in mind. HINT: using the `uiids` and `uuids` lists or the corresponding maps may be useful here."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 68, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### 3.2 Carry out the ridge regression!"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Now we get the design matrix for the training and validation sets. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 69, 
      "cell_type": "code", 
      "source": [
        "designm, ratings = getmats(traindf)\n", 
        "validatedm, validaterats = getmats(validatedf)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "Set up a bunch of $\\alpha$s `[0.01, 0.1, 10, 100, 1000]` for our ridge regression, and carry it out. Notice that we are doing just one validation set here, rather than cross-validation, in the interests of saving on running time in this homework problem. (The Learning a Model lab might help you in your code here). \n", 
        "\n", 
        "Save in a dictionary `vdict`(validation-dict) keyed by the values of $\\alpha$ above the root-mean-square-error(rmse) on the predictions, and in dictionary `rdict`(regression-dict) with the same keys the sklearn ridge regression instances for each $\\alpha$."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 70, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Lets find the $\\alpha$ that minimizes the rmse's in vdict, and save in `regr` the corresponding regression"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 71, 
      "cell_type": "code", 
      "source": [
        "minerroralpha=min(vdict, key=vdict.get)\n", 
        "print minerroralpha\n", 
        "regr=rdict[minerroralpha] "
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "And see our intercept and how it compares to $\\bar Y$."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 72, 
      "cell_type": "code", 
      "source": [
        "regr.intercept_, ybar"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We save our intercept and coefficients into a training dictionary similar to the `train_avg` we saved earlier."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 73, 
      "cell_type": "code", 
      "source": [
        "meancept=regr.intercept_\n", 
        "fitratmap_u={k:regr.coef_[i] for i,k in enumerate(uuids)}\n", 
        "fitratmap_i={k:regr.coef_[i+len(uuids)] for i,k in enumerate(uiids)}"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 74, 
      "cell_type": "code", 
      "source": [
        "train_fits={'mean':meancept, 'users':fitratmap_u, 'items':fitratmap_i}"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "###Prediction!"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### 3.3 Use this model to predict on the test set."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We set up the test design matrix and get the test ratings:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 75, 
      "cell_type": "code", 
      "source": [
        "testdm, testrats = getmats(testdf)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "Predict the ratings on the test set and store the results in `predictions['baseline_r']`. Use these to make a diagram of our results using the `compare_results` function we defined and used earlier. Also predict the ratings on the validation set and save in `predictions_valid['baseline_r']`"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 76, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### 3.4 Compare the results from the simple baseline model to the regularized baseline model. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Plot both of them on the same graph using `compare_results`. HINT: use the `onsame` parameter in that function. Explain the results, both in terms of the width of the 1-standard deviation bands, and how the predictions are different from the 45 degree line."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 77, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "*your answer here*"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "##Q4: Collaborative Filtering and The Latent Factor Model"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Model Overview"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The central dogma in constructing a recommendation system using collaborative filtering is that *similar users will rate similar restaurants similarly*. Earlier we explicitly encoded this idea by using a similarity function to identify similar restaurants. We also assumed that either all users were the same (the global, maximal pooling, approach that we didnt do in this homework) or that only the current user was similar enough to make a recommendation (the user-specific approach that we did). In this section, we will use a model that allows us to identify both similar users and similar restaurants as a function of **latent factors**.\n", 
        "\n", 
        "We can think of latent factors as properties of restaurants (e.g., spiciness of food or price) that users have a positive or negative preference for. We do not observe these factors or the users' preferences directly, but we assume that they affect how users tend to rate restaurants. For example, if a restaurant serves a lot of spicy food and a user dislikes spicy food, then the restaurant would have a high \"spiciness\" factor, and the user would have a strongly negative preference, resulting in a prediction of a low rating. Note that if users have similar preferences, then according to the model, they will behave similarly, and likewise, if restaurants have similar latent factors, they will be rated similarly by similar users. Latent factors thus give us an intuitive way to specify a generative model the obeys the central dogma, while retaining a certain explanatory globality, rather than simply a notion that one user is like another.\n", 
        "\n", 
        "One issue that comes up with latent factor models is determining how many latent factors to include. There may be a number of different unmeasured properties that affect ratings in different ways -- for example, in addition to the spiciness factor above, there may also be a price factor that affects how users rate a restaurant. We can deal with the problem of choosing the number of latent factors to include in the same way we deal with choosing $K$ in a $K$-nearest neighbors problem."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "###Latent Factors by Matrix Factorization"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "In HW4 for cs109 2013, a latent factor model was implemented using the MCMC technique Gibbs Sampling (if you take AM207 you will learn much more about this). Here we shall use another technique that builds on our regularized baseline model. This technique is called matrix factorization.\n", 
        "\n", 
        "As you probably know by now, the usual method to do matrix factorization is the Singular Value Decomposition, or the SVD. However, conventional SVD is undefined when there are missing values in the matrix..in our case, the user-item matrix which has many, many missing values. \n", 
        "\n", 
        "So, how do we deal with this?\n", 
        "\n", 
        "Let $r_{um}$ denote the \"residual\" rating of user $u$ for item $m$. By residual rating we mean whats left over after fitting the baseline, so:\n", 
        "\n", 
        "$$r_{um} = Y_{um} - Y^{baseline}_{um}$$.\n", 
        "\n", 
        "Let us associate with each item $m$ a vector $\\bar q_{m}$ of length $L$, where L is the number of latent factors in the model. Similarly, lets associate with each user a vector $\\bar p_{u}$ of length $L$. $\\bar q_{m}$, either negative or positive, measures the extent to which an item(restaurant) possesses the latent factors: Low price, spicyness, etc. For the user, $\\bar p_{u}$ captures the extent of interest the user has in restaurants(items) that are high on the corresponding factors. Then we write as our model:\n", 
        "\n", 
        "$$r_{um} = \\bar q_{m}^{T} \\cdot \\bar p_{u}. $$\n", 
        "\n", 
        "Thus we are setting our residual equal to the interaction between user $u$ and item $m$, kind of, if you like, the user's overall interest in the item's characteristics [See http://www2.research.att.com/~volinsky/papers/ieeecomputer.pdf for more details..this is also a great reference on these techniques]. This is our first model where we attempt to capture the interaction between such quantities.\n", 
        "\n", 
        "This model can be illustrated by the diagram below:\n", 
        "\n", 
        "![residual](residual.png)\n", 
        "\n", 
        "To solve this model we dont want to do a direct SVD as most of the matrix elements are unknown. Instead we simply solve the minimization problem over all known rankings in the training set:\n", 
        "\n", 
        "\n", 
        "$$\\sum_{u,m \\in train} \\left( r_{um} - \\bar q_{m}^{T} \\cdot \\bar p_{u}\\right)^2$$\n", 
        "\n", 
        "Notice now that we have introduced a whole lot of unknowns into the problem: $L\\times(M+U)$ to be precise. We are running the danger of overfitting, again! And so we do what we did the last time, by penalizing these new unknown in the same way. Thus, we instead find the unknowns that minimize:\n", 
        "\n", 
        "$$\\sum_{u,m \\in train} \\left( r_{um} - \\bar q_{m}^{T} \\cdot \\bar p_{u}\\right)^2 +\\alpha \\left( \\|\\bar q_{m}\\|^{2} + \\|\\bar p_{u}\\|^{2} \\right)$$"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Now, imagine you have fixed $\\alpha$ the way we do in a cross-validation loop, and furthermore, let us say we have fixed the values of $\\bar q_{m}$. Then the formula above once again starts looking like a Ridge regression! There are $L$ $p_{u}$s for each u, and thus a total of $L \\times U$ coefficients are to be fit, and they are regularized by the second term in the formula above. Once these coefficients are fit, we can reverse the regression, and treat their values, as values of features. There are now $L \\times M$ $q_{m}$ to fit. You can see now that we can continue such an alternating process starting from some initially chosen random values of either the $p$ or the $q$. We should continue such an alternating process until the values of $\\bar q_{m}$ and $\\bar p_{u}$ do not change much from the previous step: we say that we have then reached convergence.\n", 
        "\n", 
        "So the process we are carrying out is alternating Ridge regressions until we reach convergence. It can be shown that such a process minimizes the risk. We shall not prove this here: regardless, it ought to be kind of obvious from the structure of the algorithm. This algorithm is called **Alternating Least Squares**(ALS) as it minimizes the square error first holding the $q$ constant as features and then using the $p$s fit as constant and finding the $q$s. This minimization problem can also be solved by Stochastic Gradient descent but that is beyond the scope of this class. ALS also has the advantage that it is massively parallelizable over either the users or the items.\n", 
        "\n", 
        "\n", 
        "At this point we have only solved for the residual rating. What if we want to solve for the entire rating? We know from earlier that the solution for the baseline rating is a ridge regression which minimizes the risk\n", 
        "\n", 
        "$$\\sum_{u,m} \\left (Y^{baseline}_{um} -  \\mu - \\bar \\theta \\cdot I_{u} - \\bar \\gamma \\cdot I_{m} \\right)^2 + \\alpha \\left( \\theta_{u}^2 + \\gamma_{m}^2 \\right)$$\n", 
        "\n", 
        "and gives us\n", 
        "\n", 
        "$$  Y_{um}^{baseline} =  \\mu +  \\bar \\theta \\cdot I_{u} +  \\bar \\gamma \\cdot I_{m} $$\n", 
        "\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We want to combine  $Y_{um}^{baseline}$ and $r_{um}$, and then we get:\n", 
        "\n", 
        "$$Y_{um} = Y_{um}^{baseline} + r_{um},$$\n", 
        "\n", 
        "which gives us\n", 
        "\n", 
        "$$Y_{um} = \\mu +  \\bar \\theta \\cdot I_{u} +  \\bar \\gamma \\cdot I_{m} + \\bar q_{m}^{T} \\cdot \\bar p_{u}$$.\n", 
        "\n", 
        "To solve this we need to simply minimize the risk of the entire regression, ie\n", 
        "\n", 
        "$$\\sum_{u,m} \\left( Y_{um} -  \\mu - \\bar \\theta \\cdot I_{u} -  \\bar \\gamma \\cdot I_{m} - \\bar q_{m}^{T} \\cdot \\bar p_{u} \\right)^2 + \\alpha \\left( \\theta_{u}^2 + \\gamma_{m}^2 + \\|\\bar q_{m}\\|^{2} + \\|\\bar p_{u}\\|^{2} \\right)$$\n", 
        "\n", 
        "This is just a bigger ridge regression with more unknowns! So let us go about constructing a design matrix to represent this regression, just like we did before!"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "####Constructing the design matrix"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The first observation to make, to practically solve this larger problem, is that we need to start with, at the very least, the same design matrix that we had earlier.\n", 
        "\n", 
        "The second thing to notice is that for each user, we have $L$ factors for a total of $U \\times L$ quantities. Thus, for the regression where we are fitting for the q's, or the coefficient corresponding to the item, our design matrix, which we shall call `designp`, must have $U + M + L \\times U$ features. What are these features? The first $U + M$ of these features are simply the concatenation of two Indicator vectors. \n", 
        "\n", 
        "Lets assume we are talking about the user with index 3 and item with index 5, and lets use L=2. Then, the first indicator vector has a 1 at the third (index 2) position and the second indicator has a 1 at the 5th (index 4) position. Further, we go to the 5th and 6th slots in the remaining $L \\times U$ features, and fill in the corresponding $p$'s $p_{51}$, and $p_{52}$. The number of coefficients we have, then are $U + M + L \\times U$ $q$'s. This is shown in the top part of the diagram below.\n", 
        "\n", 
        "![bigdesign](bigdesign.png)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Once we have fit for the $q$, we then use them to construct a new design matrix for the alternate regression, `designq`, in which we fit for the $p$. Now have $U + M + L \\times M$ features. The first $U + M$  features are again the concatenation of two Indicator vectors, and the remaining $L \\times M$ features are from the just fit $q$s. The number of coefficients we have are $U + M + L \\times M$ $p$'s. This is shown in the bottom part of the diagram above.\n", 
        "\n", 
        "Lets define a function `design_p` to calculate the design matrix for the first regression. Pay particular attention to the comments. Here `lshape` is the number of factors $L$, and inps is the input $p$ \"coefficient\" vector that we use to fill the features from the previous regression."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 78, 
      "cell_type": "code", 
      "source": [
        "#given the initial data frame, and the number of latent factors lshape\n", 
        "def design_p(indf, lshape, inps):\n", 
        "    #the number of columns in the added part of the feature matrix\n", 
        "    qshape=lshape*len(uuids)\n", 
        "    #the number of columns we are coming in with from the previous regression\n", 
        "    pshape=lshape*len(uiids)\n", 
        "    #the number of features from the baseline regression\n", 
        "    fshape=len(uuids)+len(uiids)\n", 
        "    #userid and itemid along with star rating from the input dataframe\n", 
        "    stvals=indf[['user_id', 'business_id', 'stars']].values\n", 
        "    #the design matrix of size N rows X M+U+L*U columns\n", 
        "    designp=np.zeros((stvals.shape[0], fshape+qshape))\n", 
        "    #ratings column vector of N rows\n", 
        "    ratings=np.zeros(stvals.shape[0])\n", 
        "    #for each row in the dataframe:\n", 
        "    for i, row in enumerate(stvals):\n", 
        "        #get userid, restaurant id and rating from the row\n", 
        "        userid=row[0]\n", 
        "        bizid=row[1]\n", 
        "        rat=row[2]\n", 
        "        #use the index corresponding to the userid and L to figure how many slots\n", 
        "        #in the design matrix to take up and where to take them up from\n", 
        "        #for e.g., if index is 2(ie third index) and L=2,this will be from index 4 on\n", 
        "        #(which is the 5th index. )\n", 
        "        posq=uuidmap[userid]*lshape\n", 
        "        putinat=fshape+posq\n", 
        "\n", 
        "        #use the index corresponding to the business id to get the indexes of the incoming p\n", 
        "        posp=uiidmap[bizid]*lshape\n", 
        "        #fill the baseline part of the design matrix in for this row\n", 
        "        designp[i,:-qshape]=np.concatenate([1*(userid==uuids), 1*(bizid==uiids)])\n", 
        "        #set the ith element of the rating vector to the rating from the matching row\n", 
        "        ratings[i]=rat\n", 
        "        #Fill L of the slots from putinat onwards to L elements from the p matrix\n", 
        "        #if the index is 4(the fifth index) and L=2, this is the 8th and 9th element of the\n", 
        "        #p coefficients\n", 
        "        designp[i,putinat:putinat+lshape]=inps[posp:posp+lshape]\n", 
        "    #return the constructed design matrix and ratings\n", 
        "    return designp, ratings"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "####4.1 Write a function to compute `designq`."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "In analogy with the function above, and with analogous signature, write a function `design_q` which takes as arguments the training dataframe `indf`, the number of factors `lshape`, and the input `inqs` coefficient vector of $q$s, and returns both the design matrix and the vector of ratings."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 79, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "### WARNING: this part might take some hours to a day to run!"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let us set the number of factors $L$. To get your code working set L to 2, and fit for just one alpha (say $\\alpha = 10$). Once you have your code working, set L to 4 and $\\alpha$ to a small grid in the vicinity of what worked for the simple regularized regression. Each regression run at a L of 4 could take longer than an hour.  **So leave a lot of time for this part**. \n", 
        "\n", 
        "If you have the time, experiment with higher values of L, seeing if your rmse decreases."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 80, 
      "cell_type": "code", 
      "source": [
        "L=2"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 81, 
      "cell_type": "code", 
      "source": [
        "print L*len(uiids), L*len(uuids)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Lets randomly initialize the initial set of p's and q's:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 82, 
      "cell_type": "code", 
      "source": [
        "initps=np.random.rand(L*len(uiids))\n", 
        "initqs=np.random.rand(L*len(uuids))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Now we write the main loop of our ALS code. We setup a set of $\\alpha$s we will scan to find the lowest cost on the validation set. For each $\\alpha$, we set the input p and q to the random initialization. We compute the design matrix `designp` using the training data frame `traindf`. We then carry out a ridge regression using the ratings and the `designp` design matrix. We save out the last $L \\times U$ coefficients of this regression, and use them to supply an `inqs` to construct a `designq` matrix for the next regression. We carry out the next regression and analogously save the fit p in `inps`.\n", 
        "\n", 
        "Before completing this iteration of the loop we calculate the root-mean-square error(rmse) between this iteration and the previous one (in this case the random initialization), on both `inps` and `inqs`. We save these numbers in `conv` which we use to check if our ALS has converged. We now move to the next iteration, and the next, until the rmse for both`inps` and `inqs` between the current and previous iterations has gone down to 0.005. When this happens we break out of the loop.\n", 
        "\n", 
        "Since `inps` is the last thing done in the loop, we repeat the ridge regression one last time to obtain final values in `inqs`. We save this regression model in `rdict2`, along with its validation rmse for predicting on `validatedf` in `vdict`. The model predicts on the validation set using the code:\n", 
        "\n", 
        "```\n", 
        "valdesignp, validaterats = design_p(validatedf, L, inps)\n", 
        "vpreds=regrp.predict(valdesignp)\n", 
        "rmse=get_rmse(validaterats, vpreds)\n", 
        "```"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "which constructs a `valdesignp` design matrix from the validation data frame and then uses the saved regression to predict on this design matrix."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 154, 
      "cell_type": "code", 
      "source": [
        "%%time\n", 
        "#NOTICE THE TWO ALPHAS BELOW AND READ THE EXPLANATION ABOVE\n", 
        "alphas=[5, 8, 10, 12, 15]\n", 
        "alphas=[10]\n", 
        "vdict2={}\n", 
        "rdict2={}\n", 
        "convdict={}\n", 
        "maxiters=100\n", 
        "for a in alphas:\n", 
        "    print \"alpha\", a\n", 
        "    inps=initps\n", 
        "    inqs=initqs\n", 
        "    sums=[]\n", 
        "    conv=[]\n", 
        "    reachedit=0\n", 
        "    for it in range(maxiters):\n", 
        "        #create design_p with inps randomly chosen\n", 
        "        designp, rats=design_p(traindf, L, inps)\n", 
        "        #fit\n", 
        "        regrp=Ridge(alpha=a).fit(designp, rats)\n", 
        "        inqsold=inqs\n", 
        "        inqs=regrp.coef_[-inqs.shape[0]:]\n", 
        "        #use regression coefficients as the new inqs\n", 
        "        designq, rats=design_q(traindf, L, inqs)\n", 
        "        regrq=Ridge(alpha=a).fit(designq, rats)\n", 
        "        inpsold=inps\n", 
        "        inps=regrq.coef_[-inps.shape[0]:]\n", 
        "        #just to see how far from 0 these are\n", 
        "        sums.append((inqs.sum(), inps.sum()))\n", 
        "        #see if the coefficients are converging\n", 
        "        pconv=get_rmse(inpsold, inps)\n", 
        "        qconv=get_rmse(inqsold, inqs)\n", 
        "        conv.append((pconv, qconv))\n", 
        "        if it > 9 and it % 10 ==0:\n", 
        "            print \"Iteration \",it, pconv, qconv\n", 
        "        reachedit=it\n", 
        "        if pconv < 0.005 and qconv < 0.005:\n", 
        "            break\n", 
        "    #fit once more using the new inps\n", 
        "    designp, rats=design_p(traindf, L, inps)\n", 
        "    regrp=Ridge(alpha=a).fit(designp, rats)\n", 
        "    #Now predict on the validation set\n", 
        "    valdesignp, validaterats = design_p(validatedf, L, inps)\n", 
        "    vpreds=regrp.predict(valdesignp)\n", 
        "    rmse=get_rmse(validaterats, vpreds)\n", 
        "    vdict2[a]=rmse\n", 
        "    rdict2[a]=regrp\n", 
        "    convdict[a]=(reachedit, conv, sums)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 155, 
      "cell_type": "code", 
      "source": [
        "vdict2"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We find the $\\alpha$ which minimizes the validation set error. We look into `rdict` to find the corresponding `sklearn` regression instance, and extract the final q's from it. We construct a last `designq` using these q's and use it to fit the ridge regression one last time. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 156, 
      "cell_type": "code", 
      "source": [
        "alphamin=min(vdict2, key=vdict2.get)\n", 
        "#Get the regression corresponding to the appropriate alpha\n", 
        "regrp=rdict2[alphamin]\n", 
        "#Extract the final q's at convergence\n", 
        "finalqs=regrp.coef_[-initqs.shape[0]:]\n", 
        "#having got final q's, use them to fit once more to get the finalps\n", 
        "designq, rats=design_q(traindf, L, finalqs)\n", 
        "regrq=Ridge(alpha=alphamin).fit(designq, rats)\n", 
        "#use the fit regression now to predict on the test set"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### 4.2 Make a prediction on the test set and plot your predictions compared to the basdeline model. Comment."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Use this regression `regrq` and the final q's `finalqs` to predict on the test set, using the final q's to construct the test set design matrix. Save the predictions in `predictions['svd']`.  Also save predictions on the validation set in `predictions_valid['svd']`. Use `compare_results` to plot the results on the test set and to compare to the baseline model with regularization. Comment on this comparison."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 157, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "*your comment here*"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We plot the rate at which our regressions converge, seeing that we could perhaps have gotten away with a less stringent convergence criterion."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 158, 
      "cell_type": "code", 
      "source": [
        "plt.plot(convdict[alphamin][1])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "##Q5 Combining results into an ensemble"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "From the Netflix Progress Prize 2007 paper (see http://brettb.net/project/papers/2007%20The%20BellKor%20solution%20to%20the%20Netflix%20prize.pdf):\n", 
        "\n", 
        ">Predictive accuracy is substantially improved when blending multiple predictors. Our experience is that most efforts should be concentrated in deriving substantially different approaches, rather than refining a single technique. Consequently, our solution is an ensemble of many methods.\n", 
        "We approach blending as a linear regression problem. We ought to regress a target ratings vector on multiple predictors. The target rating vector can be the true ratings of the Probe set, and the predictors are the respective estimates for the Probe set by an ensemble of methods. The solution is the coefficients, or the weights, that should be given to each of the predictors in the ensemble.\n", 
        "\n", 
        "\n", 
        "Finally, we shall combine the ratings obtained from the kNN and the ALS approaches."
      ], 
      "cell_type": "markdown", 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "###Adding a regularized baseline to kNN"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "To do a really good job with this one ought to have fit the knn with coefficients from scratch. But we will use a regularized baseline for the kNN instead here, and combine the two and see what we get. \n", 
        "\n", 
        "Furthermore we should have a larger validation set to this combination, as we will combine model predictions on the validation set. This is not true here either. Thus this part of the homework serves more as illustration, and more precisely as an illustration of a new ensembling technique, **stacked** regression.\n", 
        "\n", 
        "We'll approach this combination in a simple way, by carrying out a **stacked** linear regress on the *predictions* of the two models. We'll do this regression on the validation set predictions, and check our predictions on the test set.\n", 
        "\n", 
        "So we'll add one more model here, `knn_r`, which adds knn on the validation and test sets onto a baseline that is created from the regularized regression `baseline_r`."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### 5.1 Find the best fit `knn_r model` on the validation set and test sets"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Analogous to what you did in 2.5 and 2.6, find the best fit model when the baseline you choose is that from the regularized regression from question 3. Hint: Use `train_fits` for the base line instead of `train_avg` in `get_ratings_user_nbd`)."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 167, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "We find the minimum."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 168, 
      "cell_type": "code", 
      "source": [
        "mintup2=min(rmsedict2, key=rmsedict2.get)\n", 
        "mintup2"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 169, 
      "cell_type": "code", 
      "source": [
        "predictions['knn_r'], atest = get_ratings_user_nbd(testdf, traindf, train_fits, db, k=mintup2[0], reg=mintup2[1])\n", 
        "predictions_valid['knn_r'], avals = get_ratings_user_nbd(validatedf, traindf, train_fits, db, k=mintup2[0], reg=mintup2[1])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "What models have we collected so far?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 170, 
      "cell_type": "code", 
      "source": [
        "predictions.keys()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We construct a dataframe with the two predictions and the actual value on the **Validation** set."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 171, 
      "cell_type": "code", 
      "source": [
        "dfensemble=pd.DataFrame.from_dict({'knn_r':predictions_valid['knn_r'],\n", 
        "                                   'svd':predictions_valid['svd'], \n", 
        "                                   'baseline_r':predictions_valid['baseline_r'], 'y':validatedf.stars.values})"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### 5.2 Carry out an unregularized linear regression of the actual values against the three predictions"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Use the `y` values obtained from the validation set in the dataframe `dfensemble`. Regress against the other columns from the validation set. Such a regression is called 'stacking'. Store the regression model in the variable `valreg`"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 172, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### 5.3 Get the same models on the test set and use linear regression to calculate the predictions. Comment"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        " Store the predictions on the test set in the dataframe `dfensembletest` which should have columns exactly analogous to those on `dfensemble`. Use this dataframe in the prediction process on the test set. Store in the variable `epreds` the predictions of our stacked regression `valreg`, and the actual values of the ratings from the test set in the variable `testactual`. Comment on the results, using the diagram we plot below and the value of the RMSE as compared to before."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 174, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Lets plot the results and see the RMSE:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 175, 
      "cell_type": "code", 
      "source": [
        "ax=compare_results(testactual, epreds, model=\"ensemble\", predicteds=True);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "*your answer here*"
      ], 
      "cell_type": "markdown", 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "See http://web.stanford.edu/~lmackey/papers/netflix_story-nas11-slides.pdf for a fun presentation on Ensembling and the Netflix Prize. If you'd like to learn more, please read Chris Volinksy's papers on the Netflix prize. There are also comprehensive reviews [here](http://arxiv.org/abs/1202.1112) and [here](http://www.grouplens.org/system/files/FnT%20CF%20Recsys%20Survey.pdf).\n", 
        "\n", 
        "FIN"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ]
}