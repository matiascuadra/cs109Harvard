{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.10", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }, 
  "nbformat": 4, 
  "nbformat_minor": 0, 
  "cells": [
    {
      "source": [
        "# Homework 5: Part 2"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Welcome Back"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Now that you've preprocessed the data using Spark and Natural Language processing, found topics in sentences using LDA, and created some sentiment analysis using Naive Bayes we continue our analysis by looking at the probabilities from our model. \n", 
        "\n", 
        "If you used the AWS cluster or Vagrant for Spark make sure to shut down your cluster and `halt` Vagrant."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#Table of Contents\n", 
        "* [Homework 5: Part 2](#Homework-5:-Part-2)\n", 
        "\t* [Welcome Back](#Welcome-Back)\n", 
        "\t\t* [Create dataframes for Topic 0 and Topic 1](#Create-dataframes-for-Topic-0-and-Topic-1)\n", 
        "\t* [5. Playing with probability](#5.-Playing-with-probability)\n", 
        "\t\t* [Probabilities per review and topic](#Probabilities-per-review-and-topic)\n", 
        "\t\t\t* [5.1 What are the probabilities of classification for the two LDA topics?](#5.1-What-are-the-probabilities-of-classification-for-the-two-LDA-topics?)\n", 
        "\t\t* [A single example.](#A-single-example.)\n", 
        "\t\t\t* [5.2 Plots of probability vs star rating](#5.2-Plots-of-probability-vs-star-rating)\n", 
        "\t\t* [A Bayesian analysis of sentence probabilities](#A-Bayesian-analysis-of-sentence-probabilities)\n", 
        "\t\t* [Assuming Normality](#Assuming-Normality)\n", 
        "\t\t\t* [Posterior distribution](#Posterior-distribution)\n", 
        "\t\t* [Empirical Bayes](#Empirical-Bayes)\n", 
        "\t\t\t* [Marginal Distribution of data](#Marginal-Distribution-of-data)\n", 
        "\t\t* [Estimating $\\mu$](#Estimating-$\\mu$)\n", 
        "\t\t\t* [5.3 Write a function to calculate the overall mean of all reviews for a given restaurant](#5.3-Write-a-function-to-calculate-the-overall-mean-of-all-reviews-for-a-given-restaurant)\n", 
        "\t\t\t* [A better estimate of $\\mu$.](#A-better-estimate-of-$\\mu$.)\n", 
        "\t\t* [Compute the estimated $\\tau^2$](#Compute-the-estimated-$\\tau^2$)\n", 
        "\t\t\t* [Simple initial estimate of $\\tau^2$](#Simple-initial-estimate-of-$\\tau^2$)\n", 
        "\t\t* [Iterating Once More](#Iterating-Once-More)\n", 
        "\t\t\t* [Fisher Weighting (again)](#Fisher-Weighting-%28again%29)\n", 
        "\t\t\t* [Nearly There!](#Nearly-There!)\n", 
        "\t\t\t* [Some considerations with this zero-finding (optimization)](#Some-considerations-with-this-zero-finding-%28optimization%29)\n", 
        "\t\t\t* [Calculate an updated, Fisher weighted mean.](#Calculate-an-updated,-Fisher-weighted-mean.)\n", 
        "\t\t* [Back to the posterior](#Back-to-the-posterior)\n", 
        "\t\t* [Probabilities and shrinkage](#Probabilities-and-shrinkage)\n", 
        "\t\t\t* [5.4 Make a shrinkage plot](#5.4-Make-a-shrinkage-plot)\n", 
        "\t\t\t* [Shrinkage plots on individual probabilities](#Shrinkage-plots-on-individual-probabilities)\n", 
        "\t\t\t* [5.5 Extract data into the variable `data2` and make a shrinkage plot for Topic 1.](#5.5-Extract-data-into-the-variable-data2-and-make-a-shrinkage-plot-for-Topic-1.)\n", 
        "\t\t\t* [5.6 Create probability shrinkage plots.](#5.6-Create-probability-shrinkage-plots.)\n", 
        "\t\t\t* [5.7 How would one evaluate these probabilities?](#5.7-How-would-one-evaluate-these-probabilities?)\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 1, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline\n", 
        "import numpy as np\n", 
        "import scipy as sp\n", 
        "import matplotlib as mpl\n", 
        "import matplotlib.cm as cm\n", 
        "import matplotlib.pyplot as plt\n", 
        "import pandas as pd\n", 
        "pd.set_option('display.width', 500)\n", 
        "pd.set_option('display.max_columns', 100)\n", 
        "pd.set_option('display.notebook_repr_html', True)\n", 
        "import seaborn as sns\n", 
        "sns.set_style(\"whitegrid\")\n", 
        "sns.set_context(\"poster\")"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "### Create dataframes for Topic 0 and Topic 1"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We split the dataframes based on the `topic` to create two dataframes based on Topic 0 and Topic 1."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 2, 
      "cell_type": "code", 
      "source": [
        "dftouse=pd.read_csv(\"dftouse.csv\")\n", 
        "dftouse.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 3, 
      "cell_type": "code", 
      "source": [
        "c0df=dftouse[dftouse.topic==0]\n", 
        "c1df=dftouse[dftouse.topic==1]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 4, 
      "cell_type": "code", 
      "source": [
        "c0df.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "## 5. Playing with probability"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Now that we have \"average\" probabilities with standard deviations set up on a per topic and per review basis, we can see what inferences we can set up with these probabilities."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "For each of the LDA topics, and each restaurant, here we don't have a simple probability, but rather, a mean probability and a variance on this probability, \n", 
        "\n", 
        "What are these numbers?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "###Probabilities per review and topic"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "**For the following discussion, let's limit the scope to one restaurant and one topic**. We will be writing the code to calculate these quantities for each restaurant and topic, but the math will all be done for one for simplicity.\n", 
        "\n", 
        "This restaurant will have a number of reviews ($i = 1, ..., r$), each review will have a number of sentences ($n_i$) about food. \n", 
        "\n", 
        "Each such sentence will have a probability of being positive $p_{ij}$, i.e. the probability that the $j^{th}$ sentence in the $i^{th}$ review (group) is positive.\n", 
        "\n", 
        "In order to do an analysis we need to make some distributional assumptions about the data we're observing. It's hard to make any distributional assumptions about the $p_{ij}$ themselves, they take values between 0 and 1, it would be possible to assume a Beta distribution. \n", 
        "\n", 
        "But in our case, because we're interested in the average \"positivity\" of the sentences for each review we'll be looking at \n", 
        "\n", 
        "$$\n", 
        "\\bar{y}_{i} =  \\frac{1}{n_i} \\sum_{j = 1}^{n_i}p_{ij}\n", 
        "$$\n", 
        "\n", 
        "\n", 
        "Let's assume that this average is **normally distributed** for each review ($i = 1, ..., r$). This is a major assumption, we must assume that we have a sufficient number of sentences $n_i$ for each review $i = 1, ..., r$. This assumption is not always justified by our data, but we will use it anyways.\n", 
        "\n", 
        "At this point you might be doing a double take, saying the average is a number we just calculated. But note our assumptions, common to all statistical modeling.\n", 
        "\n", 
        "The $p_{ij}$ are statistical quantities to be estimated. Here the values we calculated for them are *estimates*. Similarly, the mean of the estimates is also a sample estimate. Another sample for the same-review, same restaurant (a bootstrap sample, if you like) would give slightly different estimates. So there is a quantity $\\bar{y}_{i}$ thats normally distributed and whose mean can be estimated by the sample mean we calculated.\n", 
        "\n", 
        "What about the variance of this normal. Its the variance of the sampling distribution of the means. So its the estimated variance we calculated divided by the number of sentences for each review and topic.\n", 
        "\n", 
        "This can be a bit confusing to understand. But it might help to assume that the quantities $p_{ij}$ are normally distributed with some mean and variance parameter, estimated by the review-topic mean and variance. Then we are just trying to get the sampling distribution of the mean. But we dont have to make this assumption.\n", 
        "\n", 
        "Notice what this also says about the range of possible values for $\\bar{y}_{i}$: we know by construction that it's between 0 and 1, however this modeling assumption says that it can take any values without restriction. Generally speaking this is a also bad assumption to make, but we'll see that it gives us tremendous leverage in terms of pooling information across reviews."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### 5.1 What are the probabilities of classification for the two LDA topics?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Ok, so then assume a _Gaussian distribution_ of probability means with parameter estimates the review means and variances. Now, using the CDF of the Gaussian like we did in _Homework 2_, we can calculate the _fraction of positive classifications_ for class 0 and class 1 (Use 0.5 as the threshold).\n", 
        "\n", 
        "First we define a function `prob` which takes as arguments a mean and a variance and returns the probability that a sample is positive. We will be using this function later as well.\n", 
        "\n", 
        "Notice we need to divide the variance by the count `num` in the formula below to convert from the in-sample variance to the variance of the sample mean. Put another way as compared to the paragraph above, the reason for this is that what we are interested in estimating here is not the average probability of a sentence, for say, topic 0 being positive, but rather the average probability of topic 0 in a review being positive."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 5, 
      "cell_type": "code", 
      "source": [
        "from scipy.special import erf\n", 
        "prob= lambda mu, vari, num: .5 * (1 - erf((0.5- mu) / np.sqrt(2 * vari/float(num))))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "Now write code to report the fraction of reviews that are positive on each of the LDA classes"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 6, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Explain why these answers could be very different from the `priorp` and `priorn` we calculated earlier?. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "*your answer here*"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### A single example."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "For the business id 'T2zItRCqolfzSZR2Io0OZg',let us see what our class0 and class1 probability distributions look like"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 7, 
      "cell_type": "code", 
      "source": [
        "c0df[c0df.rid=='T2zItRCqolfzSZR2Io0OZg']"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 8, 
      "cell_type": "code", 
      "source": [
        "c1df[c1df.rid=='T2zItRCqolfzSZR2Io0OZg']"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### 5.2 Plots of probability vs star rating"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "For both class 0 and class 1 , we plot the star rating against the probability means with a 1-sigma error bar. Make the plots **side-by-side** for easier comparison and jitter the vertical position (the star rating) with a bit of random noise for clarity. Make sure the x-range of the plot is from 0 to 1. Use a dot for the mean, changing the point size using the count of the number of sentences used in the review. (you will need to overlay a `plt.scatter` for this). Once again, dont forget to divide the probability variance by the count. If the variance is 0, set it to 4. This happens because there was not enough data to estimate the probabilities well so we should assume we are very diffuse in its knowledge"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 9, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 10, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Interpret the diagrams. Which probabilities would you believe more? Why?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "*your answer here*"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### A Bayesian analysis of sentence probabilities"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The main problem with our analysis, as you might have been able to see in the figure above, is that the estimates for the mean of the distribution of average review probability are (a) all over the place, and (b) can have substantial variance because in some reviews there were just not enough sentences.\n", 
        "\n", 
        "But we have enough reviews. Why not pool information across them?\n", 
        "\n", 
        "Also, in our analysis so far we have a probability distribution $p(\\text{class}=1 \\,|\\, \\text{review}, \\text{topic})$, but we got this distribution by doing point estimates for the mean and the variance. _The rest of this homework is showing you how to do a Bayesian analysis of the problem_. This means that instead of just estimating the mean of the probability for a review being positive on-average about the food, we consider this mean to be sample from an underlying distribution, which we will also assume to be a normal. And then the parameters of that distribution would have their own \"hyper-priors\" set on them. In other words, we are setting up a hierarchical model, just like the poisson-gamma model we did in the lab and that Joe talked about in lecture.\n", 
        "\n", 
        "We will not do a full Bayesian treatment, but use empirical Bayes to estimate the hyper parameters. We do not expect you to understand everything about this model, but enough to grasp the main concepts. Most of the hard parts of this section of the homework are done for you, but it is an interesting example for a lot of concepts you have seen in the lecture and in the lab. \n", 
        "\n", 
        "Back to our whole model, which now looks as follows:\n", 
        "\n", 
        "<img src=\"model.jpg\" width=640 height=600/>\n", 
        "\n", 
        "where $\\bar{y}_i$ is the mean probability that the review is positive about the food, $\\theta_i$ and $\\sigma_i$ are the parameters for the distribution describing our $\\bar{y}_i$ and $\\mu$ and $\\tau$ are the parameters of the distribution describing the $\\theta_i$. Note that we simplified our model a bit, we'll be assuming that the $\\sigma_i$ are fixed (in reality we estimated these so there should be come uncertainty). $n_i$ is the number of sentences in the review. Note that all these are for one topic. In the diagram above we've included the topic as an argument 0 or 1.\n", 
        "\n", 
        "By doing this analysis we can enhance our estimates for reviews which have very few sentences or conflicting statements. If we just regard each review by itself, we will have large error bars on these estimates. However, if we use the full Bayes model, reviews with a lot of sentences for a topic influence the estimate for $\\tau$ and $\\mu$ and thus help to reduce the error of our prediction for the short reviews, and shrink our estimates toward the overall mean. This is the power of pooling!"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Assuming Normality"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let's talk a bit more about the distributions for our hierarchical Bayes model.\n", 
        "\n", 
        "Once again, in our case, because we're interested in the average \"positivity\" of the sentences for each review we'll be looking at \n", 
        "\n", 
        "$$\n", 
        "\\bar{y}_{i} =  \\frac{1}{n_i} \\sum_{j = 1}^{n_i}p_{ij}\n", 
        "$$\n", 
        "\n", 
        "We'll say that $\\theta_i$ is the expected posterior probability of a review being positive about food. Similarly, $\\sigma_i^2$ is the review-specific variability. \n", 
        "\n", 
        "The idea is that if a particular review is very likely to write systematically positive sentences about food, we should be able to estimate that $\\theta_i$ is relatively large and $\\sigma_i^2$ is relatively small. If on the other hand, a review gives both positive and negative sentences we'll estimate $\\theta_i$ around 0.5 and a relatively large variance $\\sigma_i^2$. So, with all this in mind, let's make the following modeling assumption:\n", 
        "\n", 
        "$$\n", 
        "\\bar{y}_{i} \\,|\\, \\theta_i \\sim N(\\theta_i, \\sigma_i^2)\n", 
        "$$\n", 
        "\n", 
        "$$\n", 
        "\\theta_i \\sim N(\\mu, \\tau^2)\n", 
        "$$\n", 
        "\n", 
        "$\\theta_i$ is the parameter we were estimating by the review-topic mean earlier.\n", 
        "\n", 
        "The second of the formulae above will allow us to share information between reviews within each restaurant. To simplify the problem we'll treat $\\sigma_i^2$ as known and fixed, estimating them with \n", 
        "\n", 
        "$$\\sigma_i^2 = \\frac{1}{n_i} \\frac{1}{n_i-1} \\sum_{j = 1}^{n_i}(p_{ij} - \\bar{y}_{i})^2  = \\frac{1}{n_i} \\times \"Review-Topic\" Variance$$"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### Posterior distribution"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "After doing some math, we can calculate the posterior distribution:\n", 
        "\n", 
        "$$\n", 
        "p(\\theta_i\\,|\\,\\bar{y}_{i})\\propto p(\\bar{y}_{i}\\,|\\,\\theta_i) p(\\theta_i)\n", 
        "\\propto \\exp\\left(-\\frac{1}{2 \\sigma_i^2} \\left(\\bar{y}_{i}-\\theta_i\\right)^2\\right)  \\exp\\left(-\\frac{1}{2 \\tau^2} \\left(\\theta_i-\\mu\\right)^2\\right)\n", 
        "$$\n", 
        "\n", 
        "After some amount of algebra you'll find that this is the kernel of a normal distribution with mean \n", 
        "\n", 
        "$\\frac{1}{\\sigma^2_{\\text{post}}}\\left(\\frac{\\mu}{\\tau^2} + \\frac{\\bar{y}_{i}}{\\sigma^2_{i}}\\right)$ \n", 
        "\n", 
        "and variance \n", 
        "\n", 
        "$ \\sigma^2_{\\text{post}} = \\left(\\frac{1}{\\tau^2} + \\frac{1}{\\sigma^2_{i}}\\right)^{-1}$. \n", 
        "\n", 
        "We can simplify the mean further to see a familiar form:\n", 
        "\n", 
        "$$\n", 
        "\\mathbb{E}[\\theta_i\\,|\\,\\bar y_i, \\mu, \\sigma_i^2, \\tau^2] = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\tau^2} \\mu + \\frac{\\tau^2}{\\sigma_i^2 + \\tau^2}\\bar{y}_{i}.\n", 
        "$$\n", 
        "\n", 
        "The _posterior mean_ is a weighted average of the prior mean and the observed average. It may seem magical that the posterior distribution is Normal, we are after all simply taking two functions of $\\theta_i$, $p(\\bar{y}_{i}\\,|\\,\\theta_i)$ and $p(\\theta_i)$, manipulating things and seeing that it looks like a normal distribution. It turns out this has to do with a general concept in Bayesian statistics known as [conjugacy](https://en.wikipedia.org/wiki/Conjugate_prior). Conjugate distributions are nice because it makes these problems mathematically tractable."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Notice that we have introduced as part of our hierarchical modelling two new parameters $\\mu$ and $\\tau$. In a full Bayesian model we would put priors on them. Here, however, we wish to use Empirical Bayes!"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Empirical Bayes "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We want Empirical Bayes estimates of $\\theta_i$, in the previous section we considered the posterior mean,\n", 
        "$$\n", 
        "\\mathbb{E}[\\theta_i|y_i, \\mu, \\sigma_i^2, \\tau^2] = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\tau^2} \\mu + \\frac{\\tau^2}{\\sigma_i^2 + \\tau^2}\\bar{y}_{i},\n", 
        "$$\n", 
        "as an estimate of $\\theta_i$. But we have the problem of setting the prior parameters $(\\mu, \\tau^2)$. If we had some understanding of the data and some true prior beliefs for the distribution of the $\\theta_i$ values we might be able to set $\\mu$ and $\\tau^2$ reasonably. We saw in the [labs](https://github.com/cs109/2015lab9/blob/master/Bayes2.ipynb) that a common approach to this problem is to instead use the data to estimate these parameters. This is the Empirical Bayes (EB) approach. \n", 
        "\n", 
        "If done carefully, Empirical Bayes yields reasonable results without having to go through the process of building a prior. Generally when we want to find an Empirical Bayes estimate of the prior parameters, we look at the marginal distribution. Because our distributions were so carefully selected (i.e. normal prior, normal likelihood) it turns out that the marginal distribution of $\\bar{y}_{i}$ is also normally distributed. More detail anout this model using EB can be found at: http://www.stat.cmu.edu/~acthomas/724/Efron-Morris.pdf ."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "$$\n", 
        "p(\\bar{y}_{i}) = \\int p(\\bar{y}_{i},\\theta_i) \\ d \\theta_i = \\int p(\\bar{y}_{i}|\\theta_i)\\ p(\\theta_i) \\ d \\theta_i\n", 
        "$$\n", 
        "\n", 
        "$$\n", 
        " = \\int\\frac{1}{ \\sqrt{ 2 \\pi \\sigma_i^2}} \\exp\\left(-\\frac{1}{2 \\sigma_i^2} \\left(\\bar{y}_{i}-\\theta_i\\right)^2\\right) \\frac{1}{ \\sqrt{ 2 \\pi \\tau^2}} \\exp\\left(-\\frac{1}{2 \\tau^2} \\left(\\theta_i-\\mu\\right)^2\\right) \\ d\\theta_i\n", 
        "$$\n", 
        "\n", 
        "After some algebra (i.e. completing the square magic) you'll find that the integral has a simple form, in particular you'll find that \n", 
        "\n", 
        "$$\n", 
        "p(\\bar{y}_{i}) = \\frac{1}{ \\sqrt{ 2 \\pi (\\tau^2 + \\sigma_i^2)}} \\exp\\left(-\\frac{1}{2 (\\tau^2 + \\sigma_i^2)} \\left(\\bar{y}_{i}-\\mu\\right)^2\\right) \n", 
        "$$\n", 
        "\n", 
        "You don't need to do this now, but if you're interested - try it. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### Marginal Distribution of data"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "What's important to us right now is that marginally we can write the distribution of $\\bar{y}_{i}$ as\n", 
        "$$\n", 
        "\\bar{y}_{i} \\sim N(\\mu,\\  \\tau^2+\\sigma_i^2)\n", 
        "$$"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Estimating $\\mu$"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "This result leads to some interesting EB estimates of $\\mu$ and $\\tau^2$, in particular we can see that using $\\hat\\mu_i = \\bar{y}_{i}$ is an unbiased estimate, since\n", 
        "$$\n", 
        "\\mathbb{E}[\\bar{y}_{i}] = \\mu.\n", 
        "$$"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### 5.3 Write a function to calculate the overall mean of all reviews for a given restaurant"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Write the function `restaurant_mean` which calculates the simple overall mean of all the sentence probabilities for a particular restaurant. We will use this simple overall mean as an initial estimate of $\\mu$.\n", 
        "\n", 
        "Its signature is:\n", 
        "\n", 
        "`def restaurant_mean(rid, revids, group_means, group_counts)`\n", 
        "\n", 
        "where `rid` is the restaurant_id, `revids` are the individual review ids (you dont need these but you might find them useful for debugging). `group_means` is a list of means, one from each review. `group_counts` is the number of sentences used from each review. The function should return the average. (`np.average` is your friend)."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 11, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "We use this function you defined to create dictionaries of overall restaurant means:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 12, 
      "cell_type": "code", 
      "source": [
        "classzero_restmeans={}\n", 
        "for k,v in c0df.groupby('rid'):\n", 
        "    rmean=restaurant_mean(k, v['review_id'], v['mean'], v['count'])\n", 
        "    classzero_restmeans[k]=rmean"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 13, 
      "cell_type": "code", 
      "source": [
        "classone_restmeans={}\n", 
        "for k,v in c1df.groupby('rid'):\n", 
        "    rmean=restaurant_mean(k, v['review_id'], v['mean'], v['count'])\n", 
        "    classone_restmeans[k]=rmean"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "#### A better estimate of $\\mu$."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "But we actually have an estimate from the $r$ different reviews. It turns out we can pool these estimates together to get an much better estimate by doing _Fisher Weighting_. The idea of these weighted estimates are that estimates that have large variances should be trusted less than estimates with small variances. In this case\n", 
        "$$\n", 
        "var(\\bar{y}_{i}) = \\tau^2+\\sigma_i^2,\n", 
        "$$\n", 
        "so the weighted estimate is\n", 
        "\n", 
        "$$\n", 
        "\\hat\\mu = \\sum_{i = 1}^r w_i\\bar{y}_{i}\n", 
        "$$\n", 
        "$$\n", 
        "w_i = \\frac{(\\tau^2+\\sigma_i^2)^{-1}}{\\sum_{j = 1}^r (\\tau^2+\\sigma_j^2)^{-1}}\n", 
        "$$\n", 
        "\n", 
        "This leads to the further issue that the estimate actually depends on $\\tau^2$, so we'll need an estimate of it as well."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Compute the estimated $\\tau^2$"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Estimating $\\tau^2$ turns out to be slightly more complex. In order to get this we'll consider another quantity based on the _marginal distribution_ of $\\bar{y}_{i}$,\n", 
        "\n", 
        "$$\n", 
        "\\bar{y}_{i} \\sim N(\\mu,\\  \\tau^2+\\sigma_i^2).\n", 
        "$$\n", 
        "\n", 
        "In particular, the quantity and its distribution is\n", 
        "\n", 
        "$$\n", 
        "\\frac{(\\bar{y}_{i} - \\mu)^2}{\\tau^2+\\sigma_i^2} \\sim \\chi_1^2,\n", 
        "$$\n", 
        "where $\\chi_1^2$  is the [chi-square distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution). \n", 
        "\n", 
        "We can use this fact to get an unbiased estimate of $\\tau^2$ as \n", 
        "\n", 
        "$$\n", 
        "\\hat\\tau_i^2 = (\\bar{y}_{i} - \\mu)^2 - \\sigma_i^2.\n", 
        "$$\n", 
        "\n", 
        "This is unbiased because the expected value of a $\\chi_1^2$ is 1. Hence \n", 
        "\n", 
        "$$\n", 
        "\\frac{\\mathbb{E}[(\\bar{y}_{i} - \\mu)^2]}{\\tau^2+\\sigma_i^2} = 1\n", 
        "$$\n", 
        "$$\n", 
        "\\mathbb{E}[(\\bar{y}_{i} - \\mu)^2] = \\tau^2+\\sigma_i^2\n", 
        "$$\n", 
        "\n", 
        "So, the expected value of our estimate is $\\tau^2$:\n", 
        "\n", 
        "$$\n", 
        "\\mathbb{E}[\\hat\\tau_i^2] = \\mathbb{E}[(\\bar{y}_{i} - \\mu)^2] - \\sigma_i^2\n", 
        "$$\n", 
        "$$\n", 
        "= \\tau^2+\\sigma_i^2 - \\sigma_i^2 = \\tau^2.\n", 
        "$$\n", 
        "\n", 
        "Now, note that we'll have many estimates of $\\tau^2$, but we only need one. The simplest thing to do is to do an unweighted average of the $\\hat\\tau_i^2$ to estimate $\\tau^2$."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### Simple initial estimate of $\\tau^2$"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "This function has the signature\n", 
        "\n", 
        "`def tausq_est(df, restmeans)`\n", 
        "\n", 
        "where `df` is the dataframe for one of the topics, and `restmeans` is the dictionary pertaining to that topic which we calculated above. To do this, we implement the equation from above:\n", 
        "\n", 
        "$$\n", 
        "\\hat\\tau_i^2 = (\\bar{y}_{i} - \\mu)^2 - \\sigma_i^2.\n", 
        "$$\n", 
        "\n", 
        "Remember that in computing $\\sigma_i^2$ we must divide the in-sample variances we calculated in the last notebook by the count."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 14, 
      "cell_type": "code", 
      "source": [
        "def tausq_est(df, restmeans):\n", 
        "    diff = df['mean']-df['rid'].apply(lambda rid: restmeans[rid])\n", 
        "    return (diff*diff - df['var']/df['count'])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "Let us use this equation to estimate $\\tau^2$ estimates for both our classes:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 15, 
      "cell_type": "code", 
      "source": [
        "c0df_tausqest=tausq_est(c0df, classzero_restmeans)\n", 
        "c1df_tausqest=tausq_est(c1df, classone_restmeans)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 16, 
      "cell_type": "code", 
      "source": [
        "np.mean(c0df_tausqest <= 0.), np.mean(c1df_tausqest <= 0.)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Note that many of these estimates are negative. This is not surprising: what it means is that the within review variance is larger than the variance of the means from each review. Imagine a case in which sentences in a review are all over the place on the quality of service/ambiance etc. Imagine multiple such reviews. Then the mean probabilities from these reviews might be close while the in-review variance is large.\n", 
        "\n", 
        "We make a copy of `c0df` and `c1df` and add a column to each one of them so that we can average these estimates to estimate $\\tau^2$ for each restaurant."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 17, 
      "cell_type": "code", 
      "source": [
        "c0df_small=c0df.copy()\n", 
        "c1df_small=c1df.copy()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 18, 
      "cell_type": "code", 
      "source": [
        "c0df_small['tausqest']=c0df_tausqest\n", 
        "c1df_small['tausqest']=c1df_tausqest"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "Let's calculate the $\\tau^2$ for each restaurant and topic. If the averaged $\\tau^2$ estimate is less than 0, we set it to 0."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 19, 
      "cell_type": "code", 
      "source": [
        "classzero_resttausqs={}\n", 
        "for k,v in c0df_small.groupby('rid'):\n", 
        "    rtau=v['tausqest'].mean()\n", 
        "    if rtau <0:\n", 
        "        rtau=0\n", 
        "    classzero_resttausqs[k]=rtau"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 20, 
      "cell_type": "code", 
      "source": [
        "classone_resttausqs={}\n", 
        "for k,v in c1df_small.groupby('rid'):\n", 
        "    rtau=v['tausqest'].mean()\n", 
        "    if rtau <0:\n", 
        "        rtau=0\n", 
        "    classone_resttausqs[k]=rtau"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "### Iterating Once More"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Now we have estimations of $\\mu$ and $\\tau^2$ for each restaurant and topic. We could stop here. But as we saw with the formula for the mean earlier, Fisher weighting can give us much better estimates. So we iterate once more, Computing new $\\tau^2$ estimates and then using them to compute an updated mean. those who dont particularly care about the math can just run the code and resume reading at the **Back to the posterior** section."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### Fisher Weighting (again)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Given $r$ unbiased estimates of $\\tau^2$, we can (once again) build a better estimate with a _Fisher Weighting_ scheme. Thus let's calculate the variance of each estimate $\\hat\\tau_i^2$.\n", 
        "\n", 
        "$$\n", 
        "var(\\hat\\tau_i^2) = var\\left((\\bar{y}_{i} - \\mu)^2 - \\sigma_i^2\\right) =  var\\left((\\bar{y}_{i} - \\mu)^2\\right)\\\\\n", 
        "= var\\left(\\frac{\\tau^2+\\sigma_i^2}{\\tau^2+\\sigma_i^2}(\\bar{y}_{i} - \\mu)^2\\right)\\\\\n", 
        "= \\left(\\tau^2+\\sigma_i^2\\right)^2 \\ var\\left(\\frac{(\\bar{y}_{i} - \\mu)^2}{\\tau^2+\\sigma_i^2}\\right) = 2\\left(\\tau^2+\\sigma_i^2\\right)^2\n", 
        "$$"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We used some basic manipulations variance to arrive at this result, in particular for a constant $a$ and random variable $X$, $\\text{var}(aX + b) = \\text{var}(aX) = a^2 \\text{var}(X)$. Also the variance of a $\\chi^2_k$ random variable is $2k$.\n", 
        "\n", 
        "Hence the _Fisher Weighted_ estimate is:\n", 
        "$$\n", 
        "\\hat\\tau^2 = \\sum_{i = 1}^r w_i \\hat\\tau_i^2 = \\sum_{i = 1}^r w_i\\left((\\bar{y}_{i} - \\mu)^2 - \\sigma_i^2\\right)\\\\\n", 
        "w_i = \\frac{\\left(\\tau^2+\\sigma_i^2\\right)^{-2}}{\\sum_{j = 1}^r \\left(\\tau^2+\\sigma_j^2\\right)^{-2}}\n", 
        "$$"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### Nearly There!"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "You may have noticed that it seems we've doubled our problem here, recall that in our estimate of $\\mu$, $\\hat\\mu$, above we found that the _Fisher Weighting_ scheme actually depended on the unknown value $\\tau^2$. So we set out to estimate this unknown $\\tau^2$ using a similar method only to find out that $\\hat\\tau^2$ not only depends on $\\mu$, but it also depends on $\\tau^2$ itself!\n", 
        "\n", 
        "There are a few ways to fix this issue, notice first that the weights $w_i$ is a function of $\\tau^2$. Hence we can consider the very complex function of $\\tau^2$\n", 
        "\n", 
        "$$\n", 
        "f(\\tau^2) = \\tau^2 - \\sum_{i = 1}^r w_i(\\tau^2) \\hat\\tau_i^2\n", 
        "$$\n", 
        "\n", 
        "If we were to find the $\\tau^2$ that makes this zero it will be the solution to the _Fisher Weighted_ estimate above! \n", 
        "\n", 
        "Let us write a function which uses the data we have so far from a class-dependent input dataframe `df` and a restaurant_id `rid` to output this mathematical function. By writing a function which makes a function, we are capturing the existing data into the function we make. This construct is a common idiom in programming languages: its called a *closure*."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 21, 
      "cell_type": "code", 
      "source": [
        "def tfishmaker(subdf):\n", 
        "    davars=subdf['var']/subdf['count']\n", 
        "    tausqest=subdf['tausqest']\n", 
        "    def tfish(tausq):\n", 
        "        weights = 0.5/(tausq + davars)**2\n", 
        "        return tausq - np.average(tausqest, weights=weights)\n", 
        "    return tfish\n", 
        "    "
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We'll use a root finding method such as Newton's or the Secant method (as we dont specify derivatives here) to find the zero of this function. Here is an illustration. We might find different roots"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 22, 
      "cell_type": "code", 
      "source": [
        "tfishfunc1=tfishmaker(c0df_small[c0df_small.rid=='7Q19H5nM3oFRyCg_j0QV1A'])\n", 
        "print sp.optimize.newton(tfishfunc1, 1e-5,  maxiter=10000)\n", 
        "print sp.optimize.newton(tfishfunc1, 0.001,  maxiter=10000)\n", 
        "print sp.optimize.newton(tfishfunc1, 0.5,  maxiter=10000)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 23, 
      "cell_type": "code", 
      "source": [
        "c0df_small[c0df_small.rid=='7Q19H5nM3oFRyCg_j0QV1A']"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### Some considerations with this zero-finding (optimization)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "As with any optimization problem there are several things to take into consideration. In particular we should ask ourselves what is the range of acceptable values for the zero? In this case $\\tau^2$ represents a variance quantity, hence must be positive. We can restrict our search to positive values. Another thing to consider is starting values, this is an optimization that may or may not have one unique zero, meaning that there could be several values of $\\tau^2$ that make $f(\\tau^2) = 0$ -- based on our definition these are all legitimate estimates. In this case we should think about what exactly it means to choose a larger value of $\\tau^2$ over a smaller one. Recall the end goal is to estimate $\\theta_i$, we're proposing to look at the posterior mean\n", 
        "\n", 
        "$$\n", 
        "\\hat\\theta_i = \\mathbb{E}[\\theta_i|y_i, \\mu, \\sigma_i^2, \\tau^2] = \\frac{\\tau^2}{\\sigma_i^2 + \\tau^2} \\mu + \\frac{\\sigma_i^2}{\\sigma_i^2 + \\tau^2}\\bar{y}_{i},\n", 
        "$$\n", 
        "\n", 
        "If we choose a larger value of $\\tau^2$, we will weight the overall mean ($\\mu$) more than if we use a smaller value of $\\tau^2$. In effect we're trusting the overall mean \"positivity\" over all reviews much more when $\\tau^2$ is larger, this is higher shrinkage -- this is more conservative. We'll opt for this more conservative choice, we'll choose the larger values of $\\tau^2$ that solve $f(\\tau^2) = 0$."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We write some code to do this, with some debugging code thrown in. Notice we use the old direct-averaged $\\tau^2$'s we calculated to set an initialization. You might need to fiddle with init for all restaurant estimations to converge. Once again, if the averaged $\\tau^2$ we find is less than 0, we set it to 0. We save our outputs in `classzero_resttausqs_1` and `classone_resttausqs_1` dictionaries, the `_1` denoting this additional iteration.\n", 
        "\n", 
        "In all of these functions we handle and print out edge cases."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 24, 
      "cell_type": "code", 
      "source": [
        "classzero_resttausqs_1={}\n", 
        "counter=0\n", 
        "edgecases0=[]\n", 
        "for k,v in c0df_small.groupby('rid'):\n", 
        "    if v.shape[0]==1:\n", 
        "        rtau=0\n", 
        "    else:\n", 
        "        tfunc = tfishmaker(v)\n", 
        "        try:\n", 
        "            init=classzero_resttausqs[k]\n", 
        "            if init==0:\n", 
        "                init=0.001\n", 
        "            else:\n", 
        "                init=0.1\n", 
        "            rtau=sp.optimize.newton(tfunc, init, maxiter=10000)\n", 
        "        except:\n", 
        "            print \">>\",k, classzero_resttausqs[k], v.shape[0]\n", 
        "            #raise ValueError\n", 
        "            rtau=0.\n", 
        "            edgecases0.append((k,classzero_resttausqs[k], 100))\n", 
        "        if rtau <0:\n", 
        "            rtau=0.\n", 
        "            edgecases0.append((k,classzero_resttausqs[k], -100))\n", 
        "    classzero_resttausqs_1[k]=rtau"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 25, 
      "cell_type": "code", 
      "source": [
        "classone_resttausqs_1={}\n", 
        "edgecases1=[]\n", 
        "for k,v in c1df_small.groupby('rid'):\n", 
        "    if v.shape[0]==1:\n", 
        "        rtau=0\n", 
        "    else:\n", 
        "        tfunc = tfishmaker(v)\n", 
        "        try:\n", 
        "            init=classone_resttausqs[k]\n", 
        "            if init==0:\n", 
        "                init=0.001\n", 
        "            else:\n", 
        "                init=0.1\n", 
        "            rtau=sp.optimize.newton(tfunc, init, maxiter=10000)\n", 
        "        except:\n", 
        "            print \">>\",k, classone_resttausqs[k], v.shape[0]\n", 
        "            #raise ValueError\n", 
        "            rtau=0\n", 
        "            edgecases1.append((k,classone_resttausqs[k], 100))\n", 
        "        if rtau <0:\n", 
        "            rtau=0\n", 
        "            edgecases1.append((k,classone_resttausqs[k], -100))\n", 
        "    classone_resttausqs_1[k]=rtau"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### Calculate an updated, Fisher weighted mean."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let's call this estimate $\\hat\\tau^2$. Now that we have a value of $\\hat\\tau^2$ we can estimate $\\mu$ using the other _Fisher Weighted_ estimate $\\hat\\mu = \\sum_{i = 1}^r w_i\\bar{y}_{i}$ that we came up with earlier.\n", 
        "\n", 
        "$$\n", 
        "\\hat\\mu = \\sum_{i = 1}^r w_i\\bar{y}_{i}\n", 
        "$$\n", 
        "$$\n", 
        "w_i = \\frac{(\\tau^2+\\sigma_i^2)^{-1}}{\\sum_{j = 1}^r (\\tau^2+\\sigma_j^2)^{-1}}\n", 
        "$$"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 26, 
      "cell_type": "code", 
      "source": [
        "def updated_mean(rid, revids, tausqdict, group_means, group_vars):\n", 
        "    zeroweights=np.zeros(group_vars.shape[0])\n", 
        "    tausq=tausqdict[rid]\n", 
        "    upd_vars=tausq+group_vars\n", 
        "    if group_means.shape[0]==1:\n", 
        "        return group_means.values[0]\n", 
        "    whichisnan=np.isinf(1./upd_vars)\n", 
        "    weights=np.where(whichisnan, zeroweights, 1./upd_vars)\n", 
        "    if np.sum(weights)==0.0:\n", 
        "        print weights\n", 
        "        print \">>>\", rid, group_means.values, tausqdict[k], group_vars.values\n", 
        "        av=np.average(group_means, weights=np.ones(group_vars.shape[0]))\n", 
        "        return av\n", 
        "    av=np.average(group_means, weights=weights)\n", 
        "    if  np.isnan(av):\n", 
        "        print rid, group_means.shape[0]\n", 
        "        print \">>>\",tausq\n", 
        "        print group_means\n", 
        "        print group_vars\n", 
        "        print zip(revids,weights)\n", 
        "    return av"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "We use the function to create two dictionaries `classzero_updmeans` and similar for class one, storing the updated means there. Notice that we can now go back to estimating $\\hat\\tau^2$ using this new $\\hat \\mu$ estimate, and so on, until complete convergence. Here we'll just stop here as our main aim is to understand shrinkage."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 27, 
      "cell_type": "code", 
      "source": [
        "classzero_updmeans={}\n", 
        "for k,v in c0df.groupby('rid'):\n", 
        "    umean=updated_mean(k, v['review_id'], classzero_resttausqs_1, v['mean'], v['var']/v['count'])\n", 
        "    classzero_updmeans[k]=umean"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 28, 
      "cell_type": "code", 
      "source": [
        "classone_updmeans={}\n", 
        "for k,v in c1df.groupby('rid'):\n", 
        "    umean=updated_mean(k, v['review_id'], classone_resttausqs_1, v['mean'], v['var']/v['count'])\n", 
        "    classone_updmeans[k]=umean"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "### Back to the posterior"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We now use the formulae from before to calculate the posterior $\\theta$ and the posterior variance on those $\\theta$, writing two python functions `updated_theta` and `updated_revvar` for the purpose.\n", 
        "\n", 
        "Respectively, these are:\n", 
        "\n", 
        "$$\n", 
        "\\mathbb{E}[\\theta_i|\\bar y_i, \\mu, \\sigma_i^2, \\tau^2] = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\tau^2} \\mu + \\frac{\\tau^2}{\\sigma_i^2 + \\tau^2}\\bar{y}_{i}\n", 
        "$$\n", 
        "\n", 
        "and \n", 
        "\n", 
        "$$ \n", 
        "\\sigma^2_{i,\\ post} = \\left(\\frac{1}{\\tau^2} + \\frac{1}{\\sigma^2_{i}}\\right)^{-1}.\n", 
        "$$\n", 
        "\n", 
        "The _posterior mean_ is a weighted average of the prior mean and the observed average. Notice that both of these functions take as arguments the updated mean estimate $\\hat \\mu$ and $\\hat \\tau^2$, as well as the review mean $\\bar y_i$ and review variance $\\sigma_i^2$ as arguments to obtain the posteriors.\n", 
        "\n", 
        "We deal with some edge cases:\n", 
        "\n", 
        "- One sentence in the review, In this case the variance estimate will be Infinity. We replace it with 4, effectively infinity in this scenario but still a workable value.\n", 
        "- Negative $\\tau^2$ estimates, these are set to 0 earlier since $\\tau^2> 0$"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 29, 
      "cell_type": "code", 
      "source": [
        "def updated_theta(updmean, updtausq, revmean, revvar):\n", 
        "    if revvar==0 or updtausq==0:\n", 
        "        return updmean\n", 
        "    num = (revmean/revvar) + (updmean/updtausq)\n", 
        "    den = (1./revvar) + (1./updtausq)\n", 
        "    av = num/den\n", 
        "    if  np.isnan(av):\n", 
        "        print updtausq\n", 
        "    return av"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 30, 
      "cell_type": "code", 
      "source": [
        "def updated_revvar(updmean, updtausq, revmean, revvar):\n", 
        "    if revvar==0 and updtausq!=0:\n", 
        "        return updtausq\n", 
        "    if updtausq==0 and revvar!=0:\n", 
        "        return revvar\n", 
        "    if updtausq==0 and revvar==0:\n", 
        "        return 4#effectively infinite\n", 
        "    den = (1./revvar) + (1./updtausq)\n", 
        "    v = 1./den\n", 
        "    if  np.isnan(v):\n", 
        "        print updtausq\n", 
        "    return v\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "We use these functions in another function `doit` which takes a dictionary of updated $\\mu$ `updmeans` and updated $\\tau^2$ estimates `updtaus` to kick out the posteriors."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 31, 
      "cell_type": "code", 
      "source": [
        "def doit(df, updmeans, updtaus):\n", 
        "    dftemp=pd.DataFrame()\n", 
        "    dftemp['review_id']=df['review_id']\n", 
        "    dftemp['rid']=df['rid']\n", 
        "    dftemp['count']=df['count']\n", 
        "    dftemp['mu']=df['rid'].apply(lambda r: updmeans[r])\n", 
        "    dftemp['tau2']=df['rid'].apply(lambda r: updtaus[r])\n", 
        "    dftemp['mean']=df['mean']\n", 
        "    dftemp['var']=df['var']\n", 
        "    dftemp['thetas']=dftemp.apply(lambda row: updated_theta(row['mu'],row['tau2'],row['mean'],row['var']/row['count']), axis=1)\n", 
        "    dftemp['varthetas']=dftemp.apply(lambda row: updated_revvar(row['mu'],row['tau2'],row['mean'],row['var']/row['count']), axis=1)\n", 
        "\n", 
        "    return dftemp"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Lets do it for topic 0:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 32, 
      "cell_type": "code", 
      "source": [
        "c0df_eb=doit(c0df, classzero_updmeans, classzero_resttausqs_1)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 33, 
      "cell_type": "code", 
      "source": [
        "c0df_eb.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "And for topic 1:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 34, 
      "cell_type": "code", 
      "source": [
        "c1df_eb=doit(c1df, classone_updmeans, classone_resttausqs_1)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "### Probabilities and shrinkage"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let us see what this entire exercise has brought us."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### 5.4 Make a shrinkage plot"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Just like in Lab 9, make a shrinkage plot which scatter-plots the posterior estimates $\\theta_i$ against the review means $\\bar y_i$. Plot the 45 degree line. Make one plot for the estimates in `c0df_eb` for topic 0 and one for the topic 1 from `c1df_eb`. Do you see shrinkage? In these plots (unlike in the lab), dont bother about plotting the standard deviations, we we have too many points in each dataframe. You'll need to turn the alpha-transparency way down to see any structure."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 35, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 36, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "*your answer here*"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### Shrinkage plots on individual probabilities"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We'll use our old friend the restaurant `T2zItRCqolfzSZR2Io0OZg` and its probabilities to illustrate shrinkage. Below we provide code to make a plot of probabilities. The unshrunk probabilities are put at y=0 with some jitter. The shrunk ones are at y=1. This will make a classic shrinkage plot like in the textbooks. \n", 
        "\n", 
        "Variances of 0 due to there being only 1 sample are rescaled to being 4 since that shows huge uncertainty."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 37, 
      "cell_type": "code", 
      "source": [
        "import itertools\n", 
        "def shrinkage_plot(data):\n", 
        "    palette = itertools.cycle(sns.color_palette())\n", 
        "    with sns.axes_style('white'):\n", 
        "        for m,t, me2, te2, c in data:\n", 
        "            color=next(palette)\n", 
        "            noise=0.04*np.random.randn()\n", 
        "            noise2=0.04*np.random.randn()\n", 
        "            plt.plot([m,t],[noise,1+noise2],'o-', color=color, lw=1)\n", 
        "            if me2==0:\n", 
        "                me2=4\n", 
        "            plt.errorbar([m,t],[noise,1+noise2], xerr=[np.sqrt(me2), np.sqrt(te2)], color=color,  lw=1)\n", 
        "        plt.yticks([])\n", 
        "        plt.xlim([0,1])\n", 
        "        sns.despine(offset=-2, trim=True, left=True)\n", 
        "    return plt.gca()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "Here is the dataframe with EB estimates for this restaurant from topic 0:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 38, 
      "cell_type": "code", 
      "source": [
        "test0=c0df_eb[c0df_eb['rid']=='T2zItRCqolfzSZR2Io0OZg']\n", 
        "test0"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "And here is how we produce the plot. We zip together the data from the shrunk dataframe, the first 2 element being the unshrunk and shrunk probability estimates, and the second their variances. The final element is the counts. This data is passed to `shrinkage_plot`. Notice that the variance here is divided by the count for us."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 39, 
      "cell_type": "code", 
      "source": [
        "data=zip(test0['mean'], test0['thetas'], test0['var']/test0['count'], test0['varthetas'], test0['count'])\n", 
        "shrinkage_plot(data);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Similarly, lets find our restaurant in the topic 1 dataframe:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 40, 
      "cell_type": "code", 
      "source": [
        "test1=c1df_eb[c1df_eb['rid']=='T2zItRCqolfzSZR2Io0OZg']\n", 
        "test1"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### 5.5 Extract data into the variable `data2` and make a shrinkage plot for Topic 1."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "This is for the business `T2zItRCqolfzSZR2Io0OZg` for whom we extracted a dataframe `test1` above. Extract analogous data `data2` from `test1` and make a similar shrinkage plot. Compare and contrast the two plots."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 41, 
      "cell_type": "code", 
      "source": [
        "data2=zip(test1['mean'], test1['thetas'], test1['var']/test1['count'], test1['varthetas'], test1['count'])\n", 
        "shrinkage_plot(data2);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "*your answer here*"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### 5.6 Create probability shrinkage plots."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Using the `prob` function you defined earlier, we can calculate the decision probabilities to decide whether a review is bullish on topic 0 or on topic one. We'll make a plot to show how these probabilities change on shrinking. Write a function `prob_shrinkage_plot` to implement this plot. \n", 
        "\n", 
        "Write similar code to `shrinkage_plot`, but this time, on the y-axis plot the probabilities calculated using `prob` on the unshrunk model instead of the y=0 we did in `shrinkage_plot`. Remember to to divide the within review variance by the count.  \n", 
        "\n", 
        "And instead of y=1, use probabilities calculated using the shrunk model. You can use a count of 1 in `prob` since the posterior variance estimate already takes this into account.\n", 
        "\n", 
        "Jitter if necessary.\n", 
        "\n", 
        "Dont forget to plot the standard-deviation error-bar on the x-axis as before. The argument into this function is `data`, and it has the same form as the `data` in `shrinkage_plot`.\n", 
        "\n", 
        "The signature is, ofcourse:\n", 
        "\n", 
        "`def prob_shrinkage_plot(data):`.\n", 
        "\n", 
        "Comment and compare the results below."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 42, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The plots are called in the same way as `shrinkage_plot`:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 43, 
      "cell_type": "code", 
      "source": [
        "prob_shrinkage_plot(data);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 44, 
      "cell_type": "code", 
      "source": [
        "prob_shrinkage_plot(data2);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "*your answer here*"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### 5.7 How would one evaluate these probabilities?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Remember that LDA is an unsupervised algorithm in answering this question."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "*your answer here*"
      ], 
      "cell_type": "markdown", 
      "metadata": {
        "collapsed": true
      }
    }
  ]
}