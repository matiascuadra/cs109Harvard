{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.10", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }, 
  "nbformat": 4, 
  "nbformat_minor": 0, 
  "cells": [
    {
      "source": [
        "# HW1: Who is the best singer ever? Rihanna or Mariah?\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "<img width=600 height=400 src=\"http://static.idolator.com/uploads/2015/02/rihanna-mariah.jpg\"/>\n", 
        "\n", 
        "Billboard Magazine puts out a top 100 list of \"singles\" every week. Information from this list, as well as that from music sales, radio, and other sources is used to determine a top-100 \"singles\" of the year list. A **single** is typically one song, but sometimes can be two songs which are on one \"single\" record.\n", 
        "\n", 
        "In this homework you will scrape both Wikipedia and Billboard Magazine to try to understand how the public's taste in music has evolved. You will do this by learning about the best singers and groups from each year (distinguishing between the two groups) as determined by the Billboard top 100 charts. You will have to scrape the web pages, clean the data, and visualize the performance of the artists and their songs. Finally, you'll find some \"features\" which might predict performance on the year end charts from the weekly ones.\n", 
        "\n", 
        "(Image credit: http://www.idolator.com/7579645/rihanna-mariah-carey-vision-of-love-made-her-want-to-do-music)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## **This homework is due Thursday 24th September, at 11:59PM.**"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We will use the **LAST COMMIT** BEFORE 11:59PM on Thursday Sep 24th 2015, on the **hw1** branch, of your repository `cs109-students/githubusername-2015hw` as your submission."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The homework will help develop your skills in:\n", 
        "\n", 
        "- web page scraping\n", 
        "- data cleaning and manipulation\n", 
        "- pandas\n", 
        "- simple exploratory data visualization\n", 
        "- thinking about data and what features might be used to make predictions\n", 
        "\n", 
        "You should have worked through Lab 1 (the pandas and python parts) and Lab 2. \n", 
        "\n", 
        "The table of contents below shows youthe structure of the homework. The headings with numbers like 1.2 and 3.2 are the ones that need to be answered. At lot of code is provided for you in this homework. DO NOT simply run it. Understand it. I promise that if you dont, you will have trouble in the parts that you have to code.\n", 
        "\n", 
        "Finally, start soon. This is not a very long homework, but we are at the point in the semester that logistical issues like git flow and python running might hold you up."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#Table of Contents\n", 
        "* [HW1: Who is the best singer ever? Rihanna or Mariah?](#HW1:-Who-is-the-best-singer-ever?-Rihanna-or-Mariah?)\n", 
        "\t* [**This homework is due Thursday 24th September, at 11:59PM.**](#**This-homework-is-due-Thursday-24th-September,-at-11:59PM.**)\n", 
        "\t* [Q1. Scraping Wikipedia for Billboard Top 100.](#Q1.-Scraping-Wikipedia-for-Billboard-Top-100.)\n", 
        "\t\t* [Scraping Wikipedia for Billboard singles](#Scraping-Wikipedia-for-Billboard-singles)\n", 
        "\t\t\t* [Parsing the Billboard Wikipedia page for 1970](#Parsing-the-Billboard-Wikipedia-page-for-1970)\n", 
        "\t\t\t* [1.1 Generalize the previous: Scrape wikipedia from 1992 to 2014](#1.1-Generalize-the-previous:-Scrape-wikipedia-from-1992-to-2014)\n", 
        "\t\t\t* [1.2 Clean data and save a json file of information from the scraped files](#1.2-Clean-data-and-save-a-json-file-of-information-from-the-scraped-files)\n", 
        "\t\t* [Construct a year-song-singer dataframe from the yearly information](#Construct-a-year-song-singer-dataframe-from-the-yearly-information)\n", 
        "\t\t\t* [Construct a panel and flatten it](#Construct-a-panel-and-flatten-it)\n", 
        "\t\t\t* [Cleaning data: the correct datatypes](#Cleaning-data:-the-correct-datatypes)\n", 
        "\t* [Q2. Scraping and Constructing: Information about Artists, Bands and Genres from Wikipedia](#Q2.-Scraping-and-Constructing:-Information-about-Artists,-Bands-and-Genres-from-Wikipedia)\n", 
        "\t\t* [Scrape information about artists from wikipedia](#Scrape-information-about-artists-from-wikipedia)\n", 
        "\t\t\t* [Pulling and saving the data](#Pulling-and-saving-the-data)\n", 
        "\t\t\t* [2.1 Extract information about singers and bands](#2.1-Extract-information-about-singers-and-bands)\n", 
        "\t\t\t* [Merging this information in](#Merging-this-information-in)\n", 
        "\t\t\t* [2.2 Split out the genres](#2.2-Split-out-the-genres)\n", 
        "\t* [Q3. Exploratory Data Analysis (EDA)](#Q3.-Exploratory-Data-Analysis-%28EDA%29)\n", 
        "\t\t* [What has been the trajectory of various genres in the popular zeitgeist?](#What-has-been-the-trajectory-of-various-genres-in-the-popular-zeitgeist?)\n", 
        "\t\t\t* [3.1 Plot a horizontal bar chart of the top 30 genres](#3.1-Plot-a-horizontal-bar-chart-of-the-top-30-genres)\n", 
        "\t\t\t* [3.2 Make a small multiples plot of the 24 most popular genres](#3.2-Make-a-small-multiples-plot-of-the-24-most-popular-genres)\n", 
        "\t\t* [Who are the highest quality singers?](#Who-are-the-highest-quality-singers?)\n", 
        "\t\t\t* [3.3 What if we used a different metric?](#3.3-What-if-we-used-a-different-metric?)\n", 
        "\t\t\t* [The age at which singers achieve their top ranking.](#The-age-at-which-singers-achieve-their-top-ranking.)\n", 
        "\t\t\t* [3.4 At what year since inception do bands reach their top rankings?](#3.4-At-what-year-since-inception-do-bands-reach-their-top-rankings?)\n", 
        "\t* [Q4: Billboard Magazine: year end results from weekly results](#Q4:-Billboard-Magazine:-year-end-results-from-weekly-results)\n", 
        "\t\t* [Get the end year results, and results from all saturdays of the year.](#Get-the-end-year-results,-and-results-from-all-saturdays-of-the-year.)\n", 
        "\t\t\t* [Get the data](#Get-the-data)\n", 
        "\t\t\t* [4.1 Parse the HTML and save the song information](#4.1-Parse-the-HTML-and-save-the-song-information)\n", 
        "\t\t* [Get winning songs, and see how they do in all these weeks](#Get-winning-songs,-and-see-how-they-do-in-all-these-weeks)\n", 
        "\t\t\t* [4.2 Plot the mean rank(y axis) against the final, year-end rank(x-axis)](#4.2-Plot-the-mean-rank%28y-axis%29-against-the-final,-year-end-rank%28x-axis%29)\n", 
        "\t\t* [Add losers in and see what features may be used to distinguish winners from losers](#Add-losers-in-and-see-what-features-may-be-used-to-distinguish-winners-from-losers)\n", 
        "\t\t\t* [4.3 Make a scatterplot matrix of these features, color coded by winners/losers](#4.3-Make-a-scatterplot-matrix-of-these-features,-color-coded-by-winners/losers)\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "---"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Python depends on packages for most of its functionality; these can be either built-in (such as sys), or third-party (like all the packages below). Either way you need to import the packages you need before using them."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 3, 
      "cell_type": "code", 
      "source": [
        "# The %... is an iPython thing, and is not part of the Python language.\n", 
        "# In this case we're just telling the plotting library to draw things on\n", 
        "# the notebook, instead of on a separate window.\n", 
        "%matplotlib inline\n", 
        "# See all the \"as ...\" contructs? They're just aliasing the package names.\n", 
        "# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\n", 
        "import numpy as np\n", 
        "import scipy as sp\n", 
        "import matplotlib as mpl\n", 
        "import matplotlib.cm as cm\n", 
        "import matplotlib.pyplot as plt\n", 
        "import pandas as pd\n", 
        "import time\n", 
        "pd.set_option('display.width', 500)\n", 
        "pd.set_option('display.max_columns', 100)\n", 
        "pd.set_option('display.notebook_repr_html', True)\n", 
        "import seaborn as sns\n", 
        "sns.set_style(\"whitegrid\")\n", 
        "sns.set_context(\"poster\")"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We use [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/) to give us a nicer default color palette, with our plots being of large (`poster`) size and with a white-grid background. (see the code above)."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Q1. Scraping Wikipedia for Billboard Top 100."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "In this question you will scrape wikipedia for Billboard's top 100. You may use any python scraping software you like. We especially recommend Beautiful Soup and PyQuery."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Scraping Wikipedia for Billboard singles"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We'll be using one of two libraries to transform HTML content into Python data structures. One is [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/) and the other is [PyQuery](https://pythonhosted.org/pyquery/).\n", 
        "\n", 
        "You can choose which one you want to use.\n", 
        "\n", 
        "PyQuery is recommended for people who have used [jQuery](https://jquery.com/) before, or who feel more comfortable using CSS selectors. BeautifulSoup on the other hand might be a better  fit for people who are familiar with Python. Either package will make the job of parsing HTML documents easier."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 4, 
      "cell_type": "code", 
      "source": [
        "from pyquery import PyQuery as pq\n", 
        "from bs4 import BeautifulSoup\n", 
        "# The \"requests\" library makes working with HTTP requests easier\n", 
        "# than the built-in urllib libraries.\n", 
        "import requests"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### Parsing the Billboard Wikipedia page for 1970"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We'll use python's `requests` module to obtain the web page at http://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_1970. From this web page we'll extract the top 100 singles and their rankings. We'll then create a list of [dictionaries](https://docs.python.org/2/tutorial/datastructures.html#dictionaries), 100 of them to be precise, with entries like `{'url': '/wiki/Sugarloaf_(band)', 'ranking': 30, 'band_singer': 'Sugarloaf', 'title': 'Green-Eyed Lady'}`. If you look at that web page, you will see that there is a link for every song, from which you can get the `url` of the singer or band. We will use these links later to scrape information about the singer or band. From the listing we can also get the band or singer name `band_singer`, and `title` of the song.\n", 
        "\n", 
        "We start by making a [GET](http://www.w3schools.com/tags/ref_httpmethods.asp) request to pull in the Billboard page from Wikipedia. This is the equivalent of using your browser to visit the page.\n", 
        "\n", 
        "In fact, you should visit the link above right now and [look at the source code of the page in the developer tools](http://wickedlysmart.com/hfhtml5/devtools.html), so you can follow what we'll be doing in the next few lines (Hint: in Chrome you can just hit F12 to get the developer tools)."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 5, 
      "cell_type": "code", 
      "source": [
        "# here we access the webpage and download the content using requests\n", 
        "t1970=requests.get(\"http://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_1970\")"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Next we will need to look for the HTML _tr_ (table row) element, but only the one that has a CSS class of _wikitable_. If you look at the page source, you'll see a construct like __class=wikitable__ on the table in question. But you will also notice that none of the _tr_ elements actually have the __class__ attribute set directly on them.\n", 
        "\n", 
        "A full depth treatment of [CSS](http://www.w3schools.com/css/) [selectors](http://www.w3schools.com/cssref/css_selectors.asp) are beyond the scope of this course, but you just need to learn enough to enable you to scrape. For now you can think of it this way: children (embedded) elements inherit style properties from their parents, unless they have the property explicitly reset on their tag attributes.\n", 
        "\n", 
        "Here is a screenshot of that page with the developer tools opened in Safari. I clicked Inspect and then the word \"Title\" to make this appear.\n", 
        "\n", 
        "![1970 songs](wikipage.png)\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "**_Read this if you would like to use pyquery_** . Otherwise, skip down a bit to the part where we use beautifulSoup.\n", 
        "\n", 
        "When you request a web page, the HTTP server on the other side of the connection sends a lot of meta-information along with the page content your browser renders. Because we are only interested in the page content (i.e. the HTML elements), we request the _text_ property of the request result (the lack of parenthesis after _text_ is a hint that we're asking the object for a property, and not calling a method).\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 6, 
      "cell_type": "code", 
      "source": [
        "d_=pq(t1970.text)"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "That text is then parsed by PyQuery (which was aliased as pq) and the result is bound to the _d\\__ variable. The naming choice here is to mimic jQuery's \\$ variable. Python is very flexible with identifier names for variables, functions, etc. They can't start with numbers, and underscores at the start and end of the name are somewhat special, but otherwise you have a lot of freedom naming them. Python is case sensitive, so my_var is different from My_var, which is different from MY_VAR, etc. Also, don't use Python [keywords](https://docs.python.org/2/reference/lexical_analysis.html#keywords) or built-in function names for your variables and functions. Weird things can happen if you do...\n", 
        "\n", 
        "The construct below generates a list like structure of _tr_ elements. The __[1:]__ part at the end means that we are only interested elements from the second position to the end of the list (in Python, indexes start at zero); that is because the first _tr_ element actually holds the table header."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 8, 
      "cell_type": "code", 
      "source": [
        "d_rows=pq(d_('.wikitable tr')[1:])"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": true
      }
    }, 
    {
      "source": [
        "Next we'll be calling the _text_ method of the PyQuery object, which will return the text between the opening and closing _tr_ tags. Note that as with the _requests_ response, this object contains a lot more information about the elements than just the text, which is why we need to explicitly tell it we only want the latter.\n", 
        "\n", 
        "You might have noticed that when we wanted the text content of the HTTP request we accessed its _text_ property, but this time we called the _text_ method (the parenthesis after the property name show that we're making a method call). Libraries are created by different people and are not always consistent. Please refer to the package documentation when in doubt.\n", 
        "\n", 
        "We will now parse the elements inside the _tr_ tags to get the data we need. I'll switch to inline comments to make it easier to follow the logic."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 9, 
      "cell_type": "code", 
      "source": [
        "# Start by creating an empty list.\n", 
        "songs=[]\n", 
        "\n", 
        "# Iterate over the elements of d_rows. In this case \"r\" will\n", 
        "# receive each value from \"d_rows\" in turn.\n", 
        "for r in d_rows:\n", 
        "    # Extract the \"td\" element from the current value of r.\n", 
        "    d_td=pq(r)('td')\n", 
        "    # Get the text from the first (index zero) \"td\" element, and convert\n", 
        "    # it to an integer. If you have the page open, this is the value on\n", 
        "    # the first column of the row (i.e. the song position on the chart).\n", 
        "    ranking = int(pq(d_td[0]).text())\n", 
        "    # The second column holds an \"a\" element, so we need to extract that\n", 
        "    # before getting the text data. This is the song title.\n", 
        "    title=pq(d_td[1])('a').text()\n", 
        "    # We then get the singer name, which is the text of the third column.\n", 
        "    band_singer=pq(d_td[2])('a').text()\n", 
        "    # Along with the singer name, we also want to get the URL to her Wikipedia page,\n", 
        "    # which is held on the \"href\" attribute of the a element.\n", 
        "    # Notice that we are still looking the same element we used to get the name (index 2).\n", 
        "    band_singer_url=pq(d_td[2])('a').attr.href\n", 
        "    # Next we'll place all this information on a dictionary (also called\n", 
        "    # a map, an associative array, a hash, etc.). We will use dictionaries\n", 
        "    # a lot, so it's worth to do some quick research on this versatil Python\n", 
        "    # structure.\n", 
        "    songdict=dict(ranking=ranking, title=title, band_singer=band_singer, url=band_singer_url)\n", 
        "    # Here we're just printing the dicitonary that was created during this iteration.\n", 
        "    # Lastly, we add the dictionary with the row information to the list we\n", 
        "    # created in the begining of the cell.\n", 
        "    # Lists will also be used extensively during the course, \n", 
        "    # so you might as well do some reserach on it too.\n", 
        "    songs.append(songdict)"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "You should now have a nice list of dictionaries with all the information you needed from the Wikipedia page."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 10, 
      "cell_type": "code", 
      "source": [
        "# By the way, indexes are open on the upper bound.\n", 
        "# So songs[2:4] will give us the third and fourth elements, but not the fifth.\n", 
        "songs[2:4]"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "---\n", 
        "\n", 
        "**_Read this if you would like to use Beautiful Soup_**\n", 
        "\n", 
        "We will now show how to accomplish the same thing with BeautifulSoup. We will also take the opportunity to showcase a programming style called Functional Programming. This style is very useful to data manipulation, and it involves encapsulating your code into small, well contained functions which call each other. My way of working is to usually write my code first, after which I **refactor** pieces of code into functions which I can then reuse in the future. Functional programming is very useful and great to know, but can get some time to get used to. It is absolutely not a requirement for this class, but we would like you to know about it, and even if you don't write in this style yourself, to be able to understand your team mates code. \n", 
        "\n", 
        "If you find the following constructs confusing, feel free to ignore them for now. It should be noted that Python is not a functional language, but it does have some attributes that allow semi-functional programs to be created.\n", 
        "\n", 
        "If you want to read more about the `BeautifulSoup` functions we use, please have a look at the [documentation](http://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n", 
        "\n", 
        "The construct in the line `rows = soup.find...` below generates a list like structure of _tr_ elements. The __[1:]__ part at the end means that we are only interested elements from the second position to the end of the list (in Python, indexes start at zero); that is because the first _tr_ element actually holds the table header."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 2, 
      "cell_type": "code", 
      "source": [
        "# We'll just reuse the request object that was previously created to create a BeautifulSoup element.\n", 
        "# The latter will be the equivalent of the \"d_\" object we created before.\n", 
        "soup = BeautifulSoup(t1970.text, \"html.parser\")\n", 
        "\n", 
        "# In this line we are looking for a single \"table\" element with a class of wikitable;\n", 
        "# and then looking for all the \"tr\" elements on that table (notice the find vs find_all calls).\n", 
        "# Even though the syntax is very different from PyQuery, the end result is similar.\n", 
        "rows = soup.find(\"table\", attrs={\"class\": \"wikitable\"}).find_all(\"tr\")[1:]\n", 
        "\n", 
        "# We then define an anonymous (lambda) function whose job it is to act on\n", 
        "#each column's element in each row in the table. Lambda functions are very\n", 
        "# handy for functional programming, and the one below should be easy to follow.\n", 
        "# The function processes each field of the parameter r accordingly. It starts by\n", 
        "# transforming the first column into an integer; it then proceeds to getting the text\n", 
        "# from the second and third elements, and finally it gets the HTTP link of the third\n", 
        "# element, and returns all that in a list (notice the surrounding brackets).\n", 
        "# The function is then bound to the cleaner variable so it can be referenced later.\n", 
        "cleaner = lambda r: [int(r[0].get_text()), r[1].get_text(), r[2].get_text(), r[2].find(\"a\").get(\"href\")]\n", 
        "\n", 
        "#lambda functions are also excellent for defining one line math functions.\n", 
        "#e.g. radius = lambda x,y: np.sqrt(x*x + y*y)\n", 
        "\n", 
        "# Next we'll create a list of names that will be used as dictionary keys.\n", 
        "fields = [\"ranking\", \"title\", \"band_singer\", \"url\"]\n", 
        "\n", 
        "# We now use the lambda function to process each \"td\" element on a given row.\n", 
        "# the [... for ... in ...] construct is a list comprehension. They look weird at\n", 
        "# first but are amazingly useful and worth spending some time to learn.\n", 
        "# At a high level, thing of it as a one line \"for loop\" that aggregates the result\n", 
        "# of each iteration into a list. So once this line finished running, we will have a list\n", 
        "# of something.\n", 
        "#\n", 
        "# The dict function is another way to create a dictionary. One neat thing about it\n", 
        "# is that it accepts a list of key/value pairs that will be used to create said dictionary.\n", 
        "#\n", 
        "# But where are these key/value pairs coming from in here? From the zip function!\n", 
        "# The zip function will take multiple iterables (things that can be treated as a sequence)\n", 
        "# and combine them. An example might make it clearer:\n", 
        "#\n", 
        "# zip([\"a\", \"b\", \"c\"], [1, 2, 3]) evaluates to [(\"a\", 1), (\"b\", 2), (\"c\", 3)]. It's like a zipper!!!\n", 
        "#\n", 
        "# Anyway, never mind the parenthesis around the pairs; they just show that the elements\n", 
        "# are grouped into tuples, which you can think of as lists that are immutable (they can't grow or shrink).\n", 
        "#\n", 
        "# So to recap: the zip function creates a list of pairs; which the dict function then uses\n", 
        "# to create a dictionary, using the first element of the pair as the key and the second as\n", 
        "# the value; and finally, the list comprehension iterates over each row element, and puts\n", 
        "# the result of each iteration on a list, which is then bound to the songs variable.\n", 
        "songs = [dict(zip(fields, cleaner(row.find_all(\"td\")))) for row in rows]"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The cell above has a lot of comments, but only 5 lines of Python code. Functional programs have a tendency to be more concise and, once you get the gist of it, easier to understand."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 13, 
      "cell_type": "code", 
      "source": [
        "songs[2:4]"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "In the end, we have a list with the same contents as the one created by the PyQuery version. On the next section you will have to parse the tables for multiple years. Pick one of the approaches presented above and adapt it to solve that problem. Or come up with your own solution!"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### 1.1 Generalize the previous: Scrape wikipedia from 1992 to 2014"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "By visiting the urls similar to the ones for 1970, we can obtain the billboard top 100 for the years 1992 to 2014. Download these using `requests` and store the text from those requests in a dictionary called `yearstext`. This dictionary ought to have as its keys the years (as integers from 1992 to 2014), and as values corresponding to these keys the text of the page being fetched.\n", 
        "\n", 
        "You ought to sleep a second at the very least in-between fetching each web page: you do not want Wikipedia to think you are a marauding bot attempting to mount a denial-of-service attack."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "*Here are some notes for how you may do this*:\n", 
        "\n", 
        "You can start by using [`range`](https://docs.python.org/2/library/functions.html#range) to create a list of numbers from 1992 to 2014 (in its simplest form the `range` function returns a list of numbers from 0 to the argument given minus one). On a side note, if you need to generate a huge list of numbers (over thousands of numbers), use the _xrange_ function instead.\n", 
        "\n", 
        "You can then use [string interpolation](https://docs.python.org/2/library/stdtypes.html#string-formatting) to request the page for each year from 1992 to 2014. These pages are added to a dictionary with the year as the key and the page text as the value.\n", 
        "\n", 
        "The `time.sleep` function can be used to wait one second between requests, otherwise Wikipedia might get mad at us.\n", 
        "\n", 
        "At the end of the cell below, the dictionary `yearstext` **must** be defined."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 14, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### 1.2 Clean data and save a json file of information from the scraped files"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Remember the code you wrote to get data from 1970 which produces a list of dictionaries, one corresponding to each single? Write a function `parse_year(year, yeartextdict)` which takes the year, prints it out, gets the text for the year from the just created year text dictionary, and return a list of dictionaries for that year, with one dictionary for each single.\n", 
        "\n", 
        "The dictionaries **must** be of this form:\n", 
        "\n", 
        "```\n", 
        "{'band_singer': ['Brandy', 'Monica'],\n", 
        "  'ranking': 2,\n", 
        "  'song': ['The Boy Is Mine'],\n", 
        "  'songurl': ['/wiki/The_Boy_Is_Mine_(song)'],\n", 
        "  'titletext': '\" The Boy Is Mine \"',\n", 
        "  'url': ['/wiki/Brandy_Norwood', '/wiki/Monica_(entertainer)']}\n", 
        "```"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Notice that some singles might have multiple songs:\n", 
        "\n", 
        "```\n", 
        "{'band_singer': ['Jewel'],\n", 
        "  'ranking': 2,\n", 
        "  'song': ['Foolish Games', 'You Were Meant for Me'],\n", 
        "  'songurl': ['/wiki/Foolish_Games',\n", 
        "   '/wiki/You_Were_Meant_for_Me_(Jewel_song)'],\n", 
        "  'titletext': '\" Foolish Games \" / \" You Were Meant for Me \"',\n", 
        "  'url': ['/wiki/Jewel_(singer)']}\n", 
        "```"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "And some don't have a song URL:\n", 
        "\n", 
        "```\n", 
        "{'band_singer': [u'Nu Flavor'],\n", 
        "  'ranking': 91,\n", 
        "  'song': [u'Heaven'],\n", 
        "  'songurl': [None],\n", 
        "  'titletext': u'\"Heaven\"',\n", 
        "  'url': [u'/wiki/Nu_Flavor']}\n", 
        "```"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "There are some additional issues this function must handle:\n", 
        "\n", 
        "1. There can be more than one one `band_singer` as can be seen above (sometimes with a comma, sometimes with \"featuring\" in between). The best way to parse these is to look for the urls.\n", 
        "2. There can be 2 songs in a single, because of the way the industry works: there are 2 sided singles. See https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_1997 for example. You can find other examples in 1998 and 1999.\n", 
        "3. The `titletext` is the contents of the table cell, and retains the quotes that wikipedia puts on the single.\n", 
        "4. If no song anchor is found (see the 24th song in the above url), assume there is one song in the single, set `songurl` to [`None`] and the song name to the contents of the table cell with the quotes stripped (ie `song` is a one-element list with this the `titletext` stripped of its quotes)."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 1, 
      "cell_type": "code", 
      "source": [
        "\"\"\"\n", 
        "Function\n", 
        "--------\n", 
        "parse_year\n", 
        "\n", 
        "Inputs\n", 
        "------\n", 
        "year: the year you want the singles for\n", 
        "ytextdixt: a dictionary with keys as integer years and values the downloaded web pages \n", 
        "    from wikipedia for that year.\n", 
        "   \n", 
        "Returns\n", 
        "-------\n", 
        "\n", 
        "a list of dictionaries, each of which corresponds to a single and has the\n", 
        "following data:\n", 
        "\n", 
        "Eg:\n", 
        "\n", 
        "{'band_singer': ['Brandy', 'Monica'],\n", 
        "  'ranking': 2,\n", 
        "  'song': ['The Boy Is Mine'],\n", 
        "  'songurl': ['/wiki/The_Boy_Is_Mine_(song)'],\n", 
        "  'titletext': '\" The Boy Is Mine \"',\n", 
        "  'url': ['/wiki/Brandy_Norwood', '/wiki/Monica_(entertainer)']}\n", 
        "  \n", 
        "A dictionary with the following data:\n", 
        "    band_singer: a list of bands/singers who made this single\n", 
        "    song: a list of the titles of songs on this single\n", 
        "    songurl: a list of the same size as song which has urls for the songs on the single \n", 
        "        (see point 3 above)\n", 
        "    ranking: ranking of the single\n", 
        "    titletext: the contents of the table cell\n", 
        "    band_singer: a list of bands or singers on this single\n", 
        "    url: a list of wikipedia singer/band urls on this single: only put in the part \n", 
        "        of the url from /wiki onwards\n", 
        "    \n", 
        "\n", 
        "Notes\n", 
        "-----\n", 
        "See description and example above.\n", 
        "\"\"\"\n", 
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "As a test, try the following:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 13, 
      "cell_type": "code", 
      "source": [
        "parse_year(1997, yearstext)[:5]"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "This should give the following. Notice that the year 1997 exercises the edge cases we talked about earlier."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "[{'band_singer': ['Elton John'],\n", 
        "  'ranking': 1,\n", 
        "  'song': ['Something About the Way You Look Tonight',\n", 
        "   'Candle in the Wind 1997'],\n", 
        "  'songurl': ['/wiki/Something_About_the_Way_You_Look_Tonight',\n", 
        "   '/wiki/Candle_in_the_Wind_1997'],\n", 
        "  'titletext': '\" Something About the Way You Look Tonight \" / \" Candle in the Wind 1997 \"',\n", 
        "  'url': ['/wiki/Elton_John']},\n", 
        " {'band_singer': ['Jewel'],\n", 
        "  'ranking': 2,\n", 
        "  'song': ['Foolish Games', 'You Were Meant for Me'],\n", 
        "  'songurl': ['/wiki/Foolish_Games',\n", 
        "   '/wiki/You_Were_Meant_for_Me_(Jewel_song)'],\n", 
        "  'titletext': '\" Foolish Games \" / \" You Were Meant for Me \"',\n", 
        "  'url': ['/wiki/Jewel_(singer)']},\n", 
        " {'band_singer': ['Puff Daddy', 'Faith Evans', '112'],\n", 
        "  'ranking': 3,\n", 
        "  'song': [\"I'll Be Missing You\"],\n", 
        "  'songurl': ['/wiki/I%27ll_Be_Missing_You'],\n", 
        "  'titletext': '\" I\\'ll Be Missing You \"',\n", 
        "  'url': ['/wiki/Sean_Combs', '/wiki/Faith_Evans', '/wiki/112_(band)']},\n", 
        " {'band_singer': ['Toni Braxton'],\n", 
        "  'ranking': 4,\n", 
        "  'song': ['Un-Break My Heart'],\n", 
        "  'songurl': ['/wiki/Un-Break_My_Heart'],\n", 
        "  'titletext': '\" Un-Break My Heart \"',\n", 
        "  'url': ['/wiki/Toni_Braxton']},\n", 
        " {'band_singer': ['Puff Daddy', 'Mase'],\n", 
        "  'ranking': 5,\n", 
        "  'song': [\"Can't Nobody Hold Me Down\"],\n", 
        "  'songurl': ['/wiki/Can%27t_Nobody_Hold_Me_Down'],\n", 
        "  'titletext': '\" Can\\'t Nobody Hold Me Down \"',\n", 
        "  'url': ['/wiki/Sean_Combs', '/wiki/Mase']}]"
      ], 
      "cell_type": "raw", 
      "metadata": {}
    }, 
    {
      "source": [
        "Then, let us store the information for each year in a dictionary `yearinfo` keyed by year."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 14, 
      "cell_type": "code", 
      "source": [
        "yearinfo = {y:parse_year(y, yearstext) for y in years}"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We would not want to lose all this work, so let's save the last data structure we created to disk. That way if you need to re-run sections 1.3 and beyond, you don't need to redo all these requests and parsing. Notice that we save into the `tempdata` folder. If you look at the `.gitignore` for this repository, you will see that files in `tempdata` are not committed...these are large files, and when you commit and push your homework, we do not need to see them..."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 4, 
      "cell_type": "code", 
      "source": [
        "import json"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 17, 
      "cell_type": "code", 
      "source": [
        "fd = open(\"tempdata/yearinfo.json\",\"w\")\n", 
        "json.dump(yearinfo, fd)\n", 
        "fd.close()"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Now let's reload our JSON file into the yearinfo variable, just to be sure everything is working."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 5, 
      "cell_type": "code", 
      "source": [
        "# Another way to deal with files. Has the advantage of closing the file for you.\n", 
        "del yearinfo\n", 
        "with open(\"tempdata/yearinfo.json\", \"r\") as fd:\n", 
        "    yearinfo = json.load(fd)"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "### Construct a year-song-singer dataframe from the yearly information"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let's construct a dataframe from the `yearinfo`. Pandas DataFrames are like Excel spreadsheets or CSV files. They are great for representing data that has the same data type on each column, but different types across columns. However, if you look at the data structure we have so far, a given key can have a list of values with multiple entries. Also, our data is grouped by year. So we need a way to flatten this data into a format that will create a useful DataFrame."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### Construct a panel and flatten it"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "To do this, we iterate over the years and the singles per year. For each single, we then iterate over singers and songs, in the end constructing a dataframe for each year. This dataframe has one row per single/singer/song combination. We can then store these dataframes into a dictionary, construct a Panel from it (look it up in the Pandas docs), and flatten the panel to get a hierarchically indexed dataframe.\n", 
        "\n", 
        "A Panel is a 3-D version of a DataFrame, or a DataFrame of DataFrames. Think of dataframes stacked on top of each other and coming out of the screen. We then flatten this Panel into a single Dataframe by using Pandas ability to have hierarchical indexing: in this case its the ranking/year combination that things get flattened under.\n", 
        "\n", 
        "Don't worry if you don't get the Panel to DataFrame part. We will have plenty of time to learn about Pandas and its data structures. But make sure you understand the logic of everything before the last three lines."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 6, 
      "cell_type": "code", 
      "source": [
        "yeardict={}\n", 
        "for y in yearinfo.keys():\n", 
        "    yearlist=yearinfo[y]\n", 
        "    yearlist2=[]\n", 
        "    for idict in yearlist:\n", 
        "        singers=idict['band_singer']\n", 
        "        for i,s in enumerate(singers):\n", 
        "            songs=idict['song']\n", 
        "            for j,so in enumerate(songs):#now inside each singer song combination\n", 
        "                nd={}\n", 
        "                nd['band_singer']=s\n", 
        "                nd['url']=idict['url'][i]\n", 
        "                nd['song']=so\n", 
        "                nd['songurl']=idict['songurl'][j]\n", 
        "                nd['ranking']=idict['ranking']\n", 
        "                yearlist2.append(nd)\n", 
        "    yeardict[y]=pd.DataFrame(yearlist2)#one for each year\n", 
        "yearspanel=pd.Panel.from_dict(yeardict, orient=\"minor\")#stack dataframes into a panel\n", 
        "hierframe=yearspanel.to_frame() #flattening leads to a hierarchical index"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": true
      }
    }, 
    {
      "execution_count": 7, 
      "cell_type": "code", 
      "source": [
        "hierframe.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We really want a non-hierarchical dataframe. So, let's flatten the dataframe further and rename some of the items in the index to create a non-hierarchically indexed dataframe."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 8, 
      "cell_type": "code", 
      "source": [
        "flatframe = hierframe.reset_index()\n", 
        "flatframe.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "execution_count": 9, 
      "cell_type": "code", 
      "source": [
        "flatframe = flatframe.rename(columns={'minor':'year'})\n", 
        "del flatframe['major']\n", 
        "flatframe.head(8)"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### Cleaning data: the correct datatypes"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Now let's check if the DataFrame has the right data types."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 10, 
      "cell_type": "code", 
      "source": [
        "flatframe.dtypes"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Pandas treats strings as generic objects, so most fields look right. However, in **1.1** we used the _range_ function to create the years, which means our dictionary should have integer keys, which should in turn become integer values on the DataFrame. What gives (hint: everything looked right until we restored the data from the JSON file)?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We should fix that. We use the [astype](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.astype.html) function to transform the data on the _year_ column from string to integer."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 11, 
      "cell_type": "code", 
      "source": [
        "flatframe.year = flatframe.year.astype(int)\n", 
        "flatframe.dtypes"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "## Q2. Scraping and Constructing: Information about Artists, Bands and Genres from Wikipedia"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Our next job is to use those singer urls and get information about singers and/or bands. Here is an example."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 25, 
      "cell_type": "code", 
      "source": [
        "for th in pq(requests.get(\"http://en.wikipedia.org/wiki/Jason_Derulo\").text)(\".infobox tr th\"):\n", 
        "    if pq(th).text() == \"Genres\":\n", 
        "        for e in pq(th).nextAll(\"td li a\"):\n", 
        "            if pq(e).attr.href.find(\"#cite_note\") == -1:\n", 
        "                print pq(e).attr.href, pq(e).attr.title"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "### Scrape information about artists from wikipedia"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We wish to fetch information about the singers or groups for all the winning songs in a list of years.\n", 
        "\n", 
        "Here we show a function that fetches information about a singer or group from their url on wikipedia. The function uses a `requests` call.\n", 
        "\n", 
        "If the request gets an HTTP code different from 200, the cells for that URL will have a value of 1; and if the request completely fails (e.g. no network connection) the cell will have a value of 2. This will allow you to analyse the failed requests.\n", 
        "\n", 
        "Notice that we have wrapped the call in whats called _an exception block_. We try to make the request. If it fails entirely, or returns a HTTP code thats not 200, we set the status to 2 and 1 respectively.\n", 
        "\n", 
        "We also create a cache object `urlcache` that will avoid redundant HTTP requests (e.g. an artist might have multiple singles on a single year, or be on the list over a span of years). Remember that this function is designed to be used in a loop over years, and then a loop over songs per year. It stands to reason that prolific singers or bands will show up again and again. Since network requests are relatively slow, if we have already getched a singer or band's wikipedia page, caching the results is a smart thing to do.\n", 
        "\n", 
        "For those interested in digging deeper into these types of optimization, check out _memoization_ techniques, and the _lru_cache_ function on the _functools_ package."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 26, 
      "cell_type": "code", 
      "source": [
        "urlcache={}\n"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": true
      }
    }, 
    {
      "execution_count": 27, 
      "cell_type": "code", 
      "source": [
        "def get_page(url):\n", 
        "    # Check if URL has already been visited.\n", 
        "    if (url not in urlcache) or (urlcache[url]==1) or (urlcache[url]==2):\n", 
        "        time.sleep(1)\n", 
        "        # try/except blocks are used whenever the code could generate an exception (e.g. division by zero).\n", 
        "        # In this case we don't know if the page really exists, or even if it does, if we'll be able to reach it.\n", 
        "        try:\n", 
        "            r = requests.get(\"http://en.wikipedia.org%s\" % url)\n", 
        "\n", 
        "            if r.status_code == 200:\n", 
        "                urlcache[url] = r.text\n", 
        "            else:\n", 
        "                urlcache[url] = 1\n", 
        "        except:\n", 
        "            urlcache[url] = 2\n", 
        "    return urlcache[url]\n"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We sort the `flatframe` by year, ascending, first. Why? It ensures that we will hit the cache most as singers who show up repeatedly in the rankings will have their information already pulled."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 28, 
      "cell_type": "code", 
      "source": [
        "flatframe=flatframe.sort('year')"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### Pulling and saving the data"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "**Note**: the function below takes a fair bit of time, about 25 minutes, for me."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 29, 
      "cell_type": "code", 
      "source": [
        "# Here we are populating the url cache\n", 
        "# subsequent calls to this cell should be very fast, since Python won't\n", 
        "# need to fetch the page from the web server.\n", 
        "#NOTE this function will take quite some time to run (about 25 mins for me), since we sleep 1 second before\n", 
        "#making a request. If you run it again it will be almost instantaneous, save requests that might have failed\n", 
        "#(you will need to run it again if requests fail..see cell below for how to test this)\n", 
        "flatframe[\"url\"].apply(get_page)"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "You may have to run this function again and again, incase there were network problems and such. Notice that because there is a \"global\" cache, it will take less time each time you run it. Also note that this function is designed to be run again and again: it just tries to make sure that there are no unresolved pages left. Let us make sure of this: *the sum below should be 0, and the boolean True.*"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 30, 
      "cell_type": "code", 
      "source": [
        "print np.sum([(urlcache[k]==1) or (urlcache[k]==2) for k in urlcache])# no one or 0's\n", 
        "print len(flatframe.url.unique())==len(urlcache)#we got all of the urls"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Let's save the `urlcache` to disk and remove the old object."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 31, 
      "cell_type": "code", 
      "source": [
        "with open(\"tempdata/artistinfo.json\",\"w\") as fd:\n", 
        "    json.dump(urlcache, fd)\n", 
        "del urlcache"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Now let's reload the dictionary from the JSON file."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 12, 
      "cell_type": "code", 
      "source": [
        "with open(\"tempdata/artistinfo.json\") as json_file:\n", 
        "    urlcache = json.load(json_file)"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### 2.1 Extract information about singers and bands"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "From each page we collected about a singer or a band, extract the following information:\n", 
        "\n", 
        "1. The genres that the band or singer works in (sidebar on right). These genres should be urls, this ensures their uniqueness. Create a list of these urls `genres`. If no genres, use `['NA']`.\n", 
        "\n", 
        "2. If the page has the text \"Born\" in the sidebar on the right, extract the element with the class `.bday`. If no \"Born\", store `False`. Store either of these into the variable `born`. We want to analyze the artist's age.\n", 
        "\n", 
        "3. If the text \"Years active\" is found, but no \"born\", assume a band. Store into the variable `ya` the value of the next table cell corresponding to this, or `False` if the text is not found.\n", 
        "\n", 
        "Put this all into a function `singer_band_info` which takes the singer/band url as argument and returns a dictionary `dict(url=url, genres=genres, born=born, ya=ya)`.\n", 
        "\n", 
        "The information can be found on the sidebar on each such wikipedia page, as the example here shows:\n", 
        "\n", 
        "![sandg](sandg.png).\n", 
        "\n", 
        "Once again, we can use the developer tools to divulge the structure of the web page:\n", 
        "\n", 
        "![diana](dianaross.png)\n", 
        "\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Please write the function `singer_band_info` according to the following specification:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 17, 
      "cell_type": "code", 
      "source": [
        "\"\"\"\n", 
        "Function\n", 
        "--------\n", 
        "singer_band_info\n", 
        "\n", 
        "Inputs\n", 
        "------\n", 
        "url: the url\n", 
        "page_text: the text associated with the url\n", 
        "   \n", 
        "Returns\n", 
        "-------\n", 
        "A dictionary with the following data:\n", 
        "    url: copy the input argument url into this value\n", 
        "    genres: the genres that the band or singer works in\n", 
        "    born: the artist's birthday\n", 
        "    ya: years active variable\n", 
        "\n", 
        "Notes\n", 
        "-----\n", 
        "See description above. Also note that some of the genres urls might require a \n", 
        "bit of care and special handling.\n", 
        "\"\"\"\n", 
        "\n", 
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### Merging this information in"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let's iterate over the items in the singer-group dictionary cache `urlcache`, run the above function, and create a dataframe from there."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 18, 
      "cell_type": "code", 
      "source": [
        "singer_band_info_list=[]\n", 
        "for k,v in urlcache.items():\n", 
        "    singer_band_info_list.append(singer_band_info(k, v))"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "execution_count": 19, 
      "cell_type": "code", 
      "source": [
        "tempdf=pd.DataFrame(singer_band_info_list)\n", 
        "tempdf.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We merge the artist/song data frames into one large dataframe. Note that this has an effect of imputing to a song **all the genres** that the artist is active in. We know that this is not true, but it is the simplest assumption we can make, and is probably good for most artists. (To think: how might you check this?)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 20, 
      "cell_type": "code", 
      "source": [
        "largedf=flatframe.merge(tempdf, on=\"url\")\n", 
        "largedf.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### 2.2 Split out the genres"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Update the dataframe by adding a column for each genre with true-false encoding for each row. This means, we want columns like \"/wiki/Acid_house\", \"/wiki/Acid_jazz\", where if an artist is in that genre, the dataframe cell has a python boolean, `True`. Otherwise, it has a `False`.\n", 
        "\n", 
        "Remember that an artist/band can be in multiple genres, and must have a \"True\" for each column corresponding to these genres, and false otherwise.\n", 
        "\n", 
        "This will widen the dataframe by the total number of genres that we have. The expanded part will look a bit like this:\n", 
        "\n", 
        "![expansion](expandingframe.png)\n", 
        "\n", 
        "HINT: for example code see http://nbviewer.ipython.org/github/cs109/content/blob/master/lec_04_wrangling.ipynb, a notebook which lays out a very nice rubric for data analysis."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 21, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "## Q3. Exploratory Data Analysis (EDA)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "At this point we have one dataframe `flatframe` for the songs, and one merged dataframe which has the songs with artist info in each row for each song, `largedf`.\n", 
        "\n", 
        "Our first visualization is an attempt to answer the following question:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### What has been the trajectory of various genres in the popular zeitgeist?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We will use the trends in the Billboard top 100 as evidence. To that goal, let us see which are the 24 most popular genres."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "To do this, we calculate the mean of the dataframe and eliminate the first two columns (`year` and `ranking`) to get means of all the genre columns. We sort it in ascending order, multiply by 100 to get percentages, and pick the top 24."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 22, 
      "cell_type": "code", 
      "source": [
        "# Get the mean of all the *numerical* values, over the column axis (0).\n", 
        "# This works because True is numerically equal to 1 in Python (e.g. True + True == 2)\n", 
        "# so mean sums up the trues and divides by the total\n", 
        "#and it only does it for numerical columns\n", 
        "genrefrac = largedf.mean(axis=0)*100\n", 
        "# The first two numerical values are ranking and year, so start from the third.\n", 
        "genrefrac = genrefrac[2:]\n", 
        "# Sort the values in descending order (note that the sort method changed the list in place).\n", 
        "#This is because its a series sort. Pandas Dataframe sorts, the type we used earlier, do not\n", 
        "#do the sort in place.\n", 
        "genrefrac.sort(ascending=False)\n", 
        "# Get the first 24 items.\n", 
        "genrefrac[:24]"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "####3.1 Plot a horizontal bar chart of the top 30 genres"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "You will have to change the index of the `genrefrac` series to strip `/wiki` from the genre names, and convert underscores to spaces. **Hint**: Use the pandas `plot`, `reset_index` and `set_index` methods. Label the y-axis \"Genres\"."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 42, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Lets take the top 24 genres from `genrefrac`. For each genre, we create a subframe of the ranking and year from those rows for which that genre is \"True\". We group the resulting dataframe by year. We obtain a dictionary `genreinfo` keyed by genre, whose value is a tuple of 3 Pandas series: the mean rankings per year for that genre, the standard deviation on those rankings, and the series groupby object for the rankings per year."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 39, 
      "cell_type": "code", 
      "source": [
        "genreinfo={}\n", 
        "topgenres=genrefrac.index[:24]\n", 
        "for genre in topgenres:\n", 
        "    genreframe=largedf[largedf[genre]==True][['ranking','year']]\n", 
        "    ggby = genreframe.groupby('year')\n", 
        "    genreinfo[genre]={'meanseries': ggby['ranking'].mean(), 'stdseries': ggby['ranking'].std(), 'rankingseries': ggby['ranking']}\n", 
        "genreinfo['/wiki/Funk']"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Notice that there are NaN's in the standard deviations. Pandas calculates standard deviations for samples rather than populations, and if there is only 1 item, then the standard deviation calculation involves a division by 0."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### 3.2 Make a small multiples plot of the 24 most popular genres"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Make a 8 row, 3 column small multiples plot with elements like:\n", 
        "\n", 
        "![Small Multiple](smmult.png)\n", 
        "\n", 
        "You will use `genreinfo`, which we just computed, to make a plot of how the \"ranking of a genre\" has changed with time. The mean rank achieved by songs in a genre forms the backbone of this plot, with the standard deviation shown, and rankings of all the songs in the genre that year shown as well, at a very low transparency. (Since a song belongs to many genres through the artist, this means that a song may be represented in multiple plots. Again, while not strictly true, this is a simplifying assumption we are making.)\n", 
        "\n", 
        "Some instructions for the plot:\n", 
        "\n", 
        "1. The blue scatters are all the songs \"in\" that genre: keep them at a low alpha so one can visually see the density. \n", 
        "2. The dark red points are the means. You can choose to join them if you like.\n", 
        "3. The light red band uses matplotlib's `fill_between` to fill the area between `mean-std` and `mean+std`. \n", 
        "4. If the standard deviation is NaN, fill up the entire y dimension with it to give a notion of large impreciseness in the measure.\n", 
        "\n", 
        "HINT: Once again, http://nbviewer.ipython.org/github/cs109/content/blob/master/lec_04_wrangling.ipynb may help in creating such a plot.\n", 
        "\n", 
        "If you have a better viz for this, by all means go for it!"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 40, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "**3.2(contd) What trends do you see?**\n", 
        "\n", 
        "Write a paragraph or two, touching on which genres are the most popular, and how this popularity has changed over time."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "*your answer here*\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Who are the highest quality singers?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Here we show the highest quality singers and plot them on a bar chart.\n", 
        "\n", 
        "What do we mean by highest quality? This is of-course open to interpretation, but lets define \"highest quality\" here as the number of times a singer appears in the top 100 over this time period. If they appear twice in a year(for different songs), this is counted as two appearances, not one. We cut our chart of at those singers who have appeared more than 15 times."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 41, 
      "cell_type": "code", 
      "source": [
        "prolific=flatframe.url.value_counts()\n", 
        "prolific[prolific > 15].plot(kind=\"barh\");"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### 3.3 What if we used a different metric?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "What we would like to capture is this: a singer ought to be scored higher if he or she appears higher in the rankings. So we'd say that a singer who appeared once at a higher and once at a lower ranking is a \"higher quality\" singer than one who appeared twice at a lower ranking. \n", 
        "\n", 
        "To do this, group all of a singers songs together and assign each song a score `101 - ranking`. Order the singers by their total score and make a bar chart for the top 20."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 42, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "**3.3(contd)Do you notice any major differences when you change the metric?**\n", 
        "\n", 
        "How have the singers at the top shifted places? Why do you think this is so?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "*your answer here*\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### The age at which singers achieve their top ranking."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "As an example to warm you up for the next question, we plot a histogram of the age at which singers achieve their top ranking"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 43, 
      "cell_type": "code", 
      "source": [
        "dfb=largedf[largedf.born!=False][['year','born','ranking','url']]\n", 
        "dfb['byear']=dfb.born.apply(lambda x: int(x.split('-')[0]))\n", 
        "byurl=dfb.groupby('url')\n", 
        "frames=[]\n", 
        "for k, v in byurl:\n", 
        "    minr=v.ranking.min()\n", 
        "    frames.append(v[v.ranking==minr])\n", 
        "topscoresdf=pd.concat(frames)\n", 
        "(topscoresdf.year-topscoresdf.byear).hist(bins=np.arange(10,70,5), normed=True);"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The *age of first fame* has an interesting shape, unimodal with a mode in the late 20s with a long tail to the right. This makes sense because there really is a lower age limit to artists but not so much an upper one. So you expect this type of behavior."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "#### 3.4 At what year since inception do bands reach their top rankings?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Make a similar calculation to plot a histogram of the years since inception at which bands reach their top ranking"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 44, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "## Q4: Billboard Magazine: year end results from weekly results"
      ], 
      "cell_type": "markdown", 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "The aim of this problem is to find features in the weekly Billboard top 100 that can predict the end year results. In the interests of time, we'll only do 2014: a complete study would do all the years.\n", 
        "Let us first fetch the end-of-year results."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Get the end year results, and results from all saturdays of the year."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 45, 
      "cell_type": "code", 
      "source": [
        "end_year=requests.get(\"http://www.billboard.com/charts/year-end/2014/hot-100-songs\").text"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Billboard releases its daily ratings on saturdays."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We first write a function `allsats(year, fs)` to get all the saturdays in a given year, and given the day part of the date of the first saturday of the year in january. We use the `date` and `timedelta` modules."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 46, 
      "cell_type": "code", 
      "source": [
        "from datetime import date, timedelta\n", 
        "\n", 
        "def allsats(year, fs):\n", 
        "    days=[]\n", 
        "    d = date(year, 1, fs)       \n", 
        "    while d.year == year:\n", 
        "        days.append(d.strftime(\"%Y-%m-%d\"))\n", 
        "        d += timedelta(days = 7)\n", 
        "    return days"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The first Saturday in 2014 is Jan 4th:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 47, 
      "cell_type": "code", 
      "source": [
        "sats=allsats(2014, 4)\n", 
        "sats"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### Get the data"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let us use `requests` to get the billboard pages for all these days. The format for the data is a dictionary `satdict` with keys the saturday date strings we see above, and one additional key, `end_year`. The values corresponding to the keys are the html text downloaded from billboard. The value corresponding to the `end_year` key comes from the `end_year` variable.\n", 
        "\n", 
        "Billboard makes the lists for every saturday available at:\n", 
        "`http://www.billboard.com/charts/hot-100/saturday`, for example, http://www.billboard.com/charts/hot-100/2014-12-27 ."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 48, 
      "cell_type": "code", 
      "source": [
        "satdict={}\n", 
        "satdict['end_year']=end_year\n", 
        "for sat in sats:\n", 
        "    satdict[sat]=requests.get(\"http://www.billboard.com/charts/hot-100/\"+sat).text\n", 
        "    time.sleep(1)"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We save the output in a file, `satdict.json`."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 49, 
      "cell_type": "code", 
      "source": [
        "with open(\"tempdata/satdict.json\",\"w\") as fd:\n", 
        "    json.dump( satdict, fd)\n", 
        "del satdict\n", 
        "with open(\"tempdata/satdict.json\") as fd:\n", 
        "    satdict=json.load(fd)"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### 4.1 Parse the HTML and save the song information"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Write a function `get_weekly(weekstring, indict)` which takes the string corresponding to a saturday, `weekstring` and a dictionary `indict` with *keys* as the saturdays and *value* as the text of the billboard page for that day. Parse the text, returning a list of tuples `(ranking, songdict)` with a `songdict` dictionary. Each tuple corresponds to a song in that week's top-100, with `ranking` the ranking of that song. \n", 
        "\n", 
        "One of these pages looks like this in the developer tools:\n", 
        "\n", 
        "![billboard](billboard.png)\n", 
        "\n", 
        "Make the dictionary `songdict` look like this: \n", 
        "\n", 
        "`songdict=dict(ranking=ranking, title=title)`\n", 
        "\n", 
        "where the title is the title of the song. The `ranking` is the same `ranking` that is the first element of the tuple."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 50, 
      "cell_type": "code", 
      "source": [
        "\"\"\"\n", 
        "Function\n", 
        "--------\n", 
        "get_weekly\n", 
        "\n", 
        "Inputs\n", 
        "------\n", 
        "weekstring: the saturday in question\n", 
        "indict: a dictionary with keys being saturdays, and values being the billboard html associated\n", 
        "with those saturdays\n", 
        "   \n", 
        "Returns\n", 
        "-------\n", 
        "A list of tuples. Each tuple has two elements. The first element is the ranking. The\n", 
        "second is a dictionary songdict with keys\n", 
        "    ranking: ranking of song. same as first element of the tuple\n", 
        "    title: title of song\n", 
        "\n", 
        "\n", 
        "Notes\n", 
        "-----\n", 
        "See description above.\n", 
        "\"\"\"\n", 
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": true
      }
    }, 
    {
      "source": [
        "Let's process the weekly and year-end information using the function you just defined. We create a `weekinfo` list with the above function run for each week. We also store in the variable `yearend` the results of running the function for the end of year rankings. We `zip` the `weekinfo` together with the saturdays to create a list of tuples `weektuples`."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 51, 
      "cell_type": "code", 
      "source": [
        "weekinfo=[]\n", 
        "for k in sats:\n", 
        "    weekinfo.append(get_weekly(k, satdict))\n", 
        "yearend=get_weekly('end_year', satdict)\n", 
        "weektuples=zip(sats, weekinfo)"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": true
      }
    }, 
    {
      "source": [
        "---\n", 
        "\n", 
        "Here we write a function `get_for_title(weektups)` that takes the tuples created above and returns a dictionary `titles` whose *key* is the title of the song, and whose *value* is a list of tuples, the members of which are (`weekindex`, `ranking`). The `weekindex` starts at 1 for the first week of the year, and `ranking` is the ranking achieved that week. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 52, 
      "cell_type": "code", 
      "source": [
        "def get_for_title(weektups):\n", 
        "    titles={}\n", 
        "    weekindex=1\n", 
        "    for weekdate, weeksrankings in weektups:\n", 
        "        for rankingtuple in weeksrankings:#iterate over the week's top-100\n", 
        "            ranking = rankingtuple[0]\n", 
        "            weekdict=rankingtuple[1]\n", 
        "            if not titles.has_key(weekdict['title']):\n", 
        "                titles[weekdict['title']]=[]\n", 
        "            titles[weekdict['title']].append((weekindex, ranking))\n", 
        "        weekindex=weekindex+1\n", 
        "    return titles"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": true
      }
    }, 
    {
      "execution_count": 53, 
      "cell_type": "code", 
      "source": [
        "titles = get_for_title(weektuples)"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Now let's create a dictionary keyed by the lower-cased title where we store the rank means (`rankmean`), rank standard deviations (`rankstd`) and rank count, the number of times(weeks) it appeared in the rankings in 2014, (`ranklen`), for a song."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 54, 
      "cell_type": "code", 
      "source": [
        "tdict={}\n", 
        "for title in titles.keys():\n", 
        "    wtlist=titles[title]\n", 
        "    weeks=[e[0] for e in wtlist]\n", 
        "    ranks=[e[1] for e in wtlist]\n", 
        "    rankmean=np.mean(ranks)\n", 
        "    rankstd=np.std(ranks, ddof=1)#numpy standard deviation is population based, make it sample based\n", 
        "    ranklen=len(ranks)\n", 
        "    tdict[title.lower()]={'ranks':(rankmean, rankstd, ranklen)}"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": true, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "### Get winning songs, and see how they do in all these weeks"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Now we create a list of winners whose elements are `dict(title, winner=1, ranking=year-end-ranking, rankmean=mean over weeks, rankstd=stddev over weeks, weekson=ranklen)`. To make this, we go to the songs that are in the top 100 at years-end and pick up their information from `tdict` above."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 55, 
      "cell_type": "code", 
      "source": [
        "winners=[]\n", 
        "for yt in yearend:\n", 
        "    r,d =yt\n", 
        "    title=d['title'].lower()\n", 
        "    rvals=tdict[title]['ranks']\n", 
        "    winners.append(dict(title=title, winner=1, ranking=d['ranking'], rankmean=rvals[0], rankstd=rvals[1], weekson=rvals[2]))"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": true
      }
    }, 
    {
      "source": [
        "And we use this list of dictionaries to create a dataframe `winners2014` with al the information gathered."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 56, 
      "cell_type": "code", 
      "source": [
        "winners2014=pd.DataFrame(winners)\n", 
        "winners2014.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### 4.2 Plot the mean rank(y axis) against the final, year-end rank(x-axis)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Show a 45 degree line for comparison. Use `plt.errorbar` to show the standard deviation along with the mean rank."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 57, 
      "cell_type": "code", 
      "source": [
        "# your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "**4.2(contd)Comment on the trend in this plot. **\n", 
        "\n", 
        "In a paragraph, tell us how the general trend compares to the 45 degree line? What does this say about the predictive ability of the average rank for the final rank?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "*your answer here*\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "###Add losers in and see what features may be used to distinguish winners from losers"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let's create a list `losers` of dictionaries, one for each song from the weekly lists that did not make it to the year end list. The dictionaries are identical to the dictionaries in the `winners` list, but we code the `ranking` as -1 for losers and set `winner` to 0. Thus"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 59, 
      "cell_type": "code", 
      "source": [
        "winlist=[e['title'] for e in winners]\n", 
        "losers=[]\n", 
        "for title in tdict:\n", 
        "    if not title in winlist:\n", 
        "        rvals=tdict[title]['ranks']\n", 
        "        losers.append(dict(title=title, winner=0, ranking=-1, rankmean=rvals[0], rankstd=rvals[1], weekson=rvals[2]))"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": true
      }
    }, 
    {
      "source": [
        "We create a `losers2014` dataframe... "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 60, 
      "cell_type": "code", 
      "source": [
        "losers2014=pd.DataFrame(losers)\n", 
        "losers2014.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "...and concatenate the losers and winners into one dataframe."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 61, 
      "cell_type": "code", 
      "source": [
        "all2014=pd.concat([winners2014, losers2014])\n", 
        "all2014.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "####4.3 Make a scatterplot matrix of these features, color coded by winners/losers"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Use `sns.pairplot`. Use all the numerical features(4), with the exception of `winner`, which you will use to give a different color to winners and losers. This will be a 4x4 matrix, with different colored dots for winners and losers off diagonal, and a 2 histograms on diagonal. The histogram shows the distribution of the one feature on the diagonal for both the winners and losers."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 62, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "scrolled": false, 
        "collapsed": false
      }
    }, 
    {
      "source": [
        "**4.3 (contd) Which combination of \"features\" is most likely to separate winners from losers? **\n", 
        "\n", 
        "It is this kind of observation in EDA that forms the basis for staring on predictive modelling! In a paragraph, identify one or two combinations of features that you think will best separate winners from losers. Why?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "*your answer here*\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        " **Until next time.**"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ]
}